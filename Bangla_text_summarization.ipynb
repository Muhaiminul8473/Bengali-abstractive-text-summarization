{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bangla_text_summarization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO0h6adTbLKfeKOd45cNg1q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhaiminul8473/Bengali-abstractive-text-summarization/blob/main/Bangla_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gohR788FIu9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a550bd2f-f2bd-4bbd-f181-b297c383a7db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCegKUuHI-0K"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_RIBpTpiyEI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkMtuLrsN-Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5e552f-5f96-47b2-be3e-d55416b64095"
      },
      "source": [
        "pip install tensorflow==1.14"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 64kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZPJwBehJQll"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epa4GHE4JjJa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "441154ad-2b5b-4a0e-d81d-015e0f2f8c66"
      },
      "source": [
        "df1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(399, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMfFmm7o-T-w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "edb3c681-430b-4392-d93d-f1e1a2823d86"
      },
      "source": [
        "df=pd.read_excel(\"/content/gdrive/My Drive/Text summarization/newtext1.xlsx\")\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>রাজধানীর মতিঝিলের শাপলা চত্বর এলাকায় আজ বৃহস্প...</td>\n",
              "      <td>শাপলা চত্বরে বাসের ধাক্কায় নিহত ১</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>রাজধানীর পুরান ঢাকার ইসলামবাগে একটি প্লাস্টিক ...</td>\n",
              "      <td>ইসলামবাগে প্লাস্টিক কারখানায় আগুন</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>আগামী ৫ জানুয়ারি অনুমতি না পেলেও রাজধানীতে বিএ...</td>\n",
              "      <td>অনুমতি না পেলেও সমাবেশ করবে বিএনপি</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>রাজধানীর ফকিরাপুলে আজ বৃহস্পতিবার সন্ধ্যা সাতট...</td>\n",
              "      <td>ফকিরাপুলে আবাসিক হোটেলে খুন\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>গত বছর মোট ৪ হাজার ৬৫৪ জন নারী বিভিন্নভাবে নির...</td>\n",
              "      <td>গত বছর ৪৬৫৪ জন নারী নির্যাতনের শিকার</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text                               Summary\n",
              "0  রাজধানীর মতিঝিলের শাপলা চত্বর এলাকায় আজ বৃহস্প...     শাপলা চত্বরে বাসের ধাক্কায় নিহত ১\n",
              "1  রাজধানীর পুরান ঢাকার ইসলামবাগে একটি প্লাস্টিক ...     ইসলামবাগে প্লাস্টিক কারখানায় আগুন\n",
              "2  আগামী ৫ জানুয়ারি অনুমতি না পেলেও রাজধানীতে বিএ...    অনুমতি না পেলেও সমাবেশ করবে বিএনপি\n",
              "3  রাজধানীর ফকিরাপুলে আজ বৃহস্পতিবার সন্ধ্যা সাতট...         ফকিরাপুলে আবাসিক হোটেলে খুন\\n\n",
              "4  গত বছর মোট ৪ হাজার ৬৫৪ জন নারী বিভিন্নভাবে নির...  গত বছর ৪৬৫৪ জন নারী নির্যাতনের শিকার"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaajTcNcC-l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c184b07a-54ff-48f9-df8c-8f007d27111f"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2731, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzI5dLm6LjIR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvj8i4yvJ8Lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aff046e-de22-4b94-d68d-c9923fd62031"
      },
      "source": [
        "\n",
        "for i in range(5):\n",
        "    print(\"News:\",i+1)\n",
        "    print(\"Text:\",df.Text[i])\n",
        "    print(\"Summary:\",df.Summary[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "News: 1\n",
            "Text: রাজধানীর মতিঝিলের শাপলা চত্বর এলাকায় আজ বৃহস্পতিবার সন্ধ্যায় যাত্রীবাহী বাসের ধাক্কায় রঞ্জন দেবনাথ (৩৯) নামের এক ব্যক্তি নিহত হয়েছেন।নিহত রঞ্জন দেবনাথের স্ত্রী বীণা বিশ্বাস ঢাকা মেডিকেল কলেজ হাসপাতালের স্টাফ নার্স। তিনি জানান, রঞ্জন দেবনাথ আফতাব গ্রুপে চাকরি করেন। \n",
            "Summary: শাপলা চত্বরে বাসের ধাক্কায় নিহত ১\n",
            "News: 2\n",
            "Text: রাজধানীর পুরান ঢাকার ইসলামবাগে একটি প্লাস্টিক পণ্য তৈরির কারখানায় আগুন লেগেছে। আজ বৃহস্পতিবার রাত ৯টা ৫৫ মিনিটে বাগানবাড়ি এলাকায় এই অগ্নিকাণ্ডের ঘটনা ঘটে।ফায়ার সার্ভিসের নিয়ন্ত্রণকক্ষে দায়িত্বরত কর্মকর্তা ফরহাদুজ্জামান প্রথম আলোকে জানান, আগুন নেভাতে ফায়ার সার্ভিসের পাঁচটি ইউনিট কাজ করছে। তবে আগুন লাগার কারণ এখনো জানা যায়নি।\n",
            "Summary: ইসলামবাগে প্লাস্টিক কারখানায় আগুন\n",
            "News: 3\n",
            "Text: আগামী ৫ জানুয়ারি অনুমতি না পেলেও রাজধানীতে বিএনপি সমাবেশ করবে বলে জানিয়েছেন দলের যুগ্ম মহাসচিব রুহুল কবির রিজভী। আজ বৃহস্পতিবার সন্ধ্যায় জানতে চাইলে প্রথম আলোকে রিজভী জানান, ‘আমরা কর্মসূচি ঘোষণা করেছি, শান্তিপূর্ণভাবে এ কর্মসূচি পালন করব।’\n",
            "Summary: অনুমতি না পেলেও সমাবেশ করবে বিএনপি\n",
            "News: 4\n",
            "Text: রাজধানীর ফকিরাপুলে আজ বৃহস্পতিবার সন্ধ্যা সাতটার দিকে একটি আবাসিক হোটেলে দুর্বৃত্তদের গুলিতে আবদুল হামিদ (৪৪) নামের এক ব্যক্তি খুন হয়েছেন। হামিদের বাড়ি চট্টগ্রাম জেলার সন্দ্বীপে।পুলিশ ও ঢাকা মেডিকেল কলেজ হাসপাতাল সূত্রে জানা যায়, আবদুল হামিদ সপ্তাহ খানেক আগে মতিঝিল থানাধীন ফকিরাপুল এলাকার হোটেল মতিঝিলের ২০১ নম্বর কক্ষ ভাড়া নেন।\n",
            "Summary: ফকিরাপুলে আবাসিক হোটেলে খুন\n",
            "\n",
            "News: 5\n",
            "Text: গত বছর মোট ৪ হাজার ৬৫৪ জন নারী বিভিন্নভাবে নির্যাতনের শিকার হয়েছেন। এর মধ্যে যৌতুকের জন্য নির্যাতনের শিকার হন ৪৩১ জন। যৌতুকের কারণে হত্যা করা হয় ২৩৬ জনকে। উত্ত্যক্তের শিকার হন ৪৬৫ জন। এঁদের মধ্যে আত্মহত্যা করেন ২১ জন। ফতোয়ার শিকার হয়েছেন ২৯ জন।\n",
            "Summary: গত বছর ৪৬৫৪ জন নারী নির্যাতনের শিকার\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u2Y3RFMScgJ"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vApsJ8nPKDXg"
      },
      "source": [
        "contractions = { \n",
        "\"বি.দ্র \": \"বিশেষ দ্রষ্টব্য\",\n",
        "\"ড.\": \"ডক্টর\",\n",
        "\"ডা.\": \"ডাক্তার\",\n",
        "\"ইঞ্জি:\": \"ইঞ্জিনিয়ার\",\n",
        "\"রেজি:\": \"রেজিস্ট্রেশন\",\n",
        "\"মি.\": \"মিস্টার\",\n",
        "\"মু.\": \"মুহাম্মদ\",\n",
        "\"মো.\": \"মোহাম্মদ\",\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip24B7VHKHTO"
      },
      "source": [
        "import re\n",
        "import string\n",
        "def clean_text(text,remove_stopwords = False):\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    # Format words and remove unwanted characters\n",
        "    whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
        "    bangla_digits = u\"[\\u09E6\\u09E7\\u09E8\\u09E9\\u09EA\\u09EB\\u09EC\\u09ED\\u09EE\\u09EF]+\"\n",
        "    english_chars = u\"[a-zA-Z0-9]\"\n",
        "    punc = u\"[(),$%^&*+={}\\[\\]:\\\"|\\'\\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+\"\n",
        "    bangla_fullstop = u\"\\u0964\"     #bangla fullstop(dari)\n",
        "    punctSeq   = u\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
        "    \n",
        "    text = re.sub(bangla_digits, \" \", text)\n",
        "    text = re.sub(punc, \" \", text)\n",
        "    text = re.sub(english_chars, \" \", text)\n",
        "    text = re.sub(bangla_fullstop, \" \", text)\n",
        "    text = re.sub(punctSeq, \" \", text)\n",
        "    text = whitespace.sub(\" \", text).strip()\n",
        "    \n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;‘:‘ ’', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]। ,', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text = re.sub(r\"[\\@$#%~+-\\.\\'।\\\"]\",\" \",text)\n",
        "    text = re.sub(r\"(?m)^\\s+\", \"\", text)\n",
        "    text = re.sub(\"[()]\",\"\",text)\n",
        "    text = re.sub(\"[‘’]\",\"\",text)\n",
        "    text = re.sub(\"[!]\",\"\",text)\n",
        "    text = re.sub(\"[/]\",\"\",text)\n",
        "    text = re.sub(\"[:]\",\"\",text)\n",
        "    text= re.sub('\\ |\\?|\\.|\\!|\\/|\\;|\\:', ' ',text)\n",
        "    text= text.strip(\"/\")\n",
        "\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bb_MuXsKPGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3962e556-9105-4767-f084-56da894da381"
      },
      "source": [
        "clean_summaries = []\n",
        "for summary in df.Summary:\n",
        "    clean_summaries.append(clean_text(summary,remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in df.Text:\n",
        "    clean_texts.append(clean_text(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HS986QxKTRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91012fb7-ae69-4ab1-92dd-d985eacf0b94"
      },
      "source": [
        "\n",
        "for i in range(5):\n",
        "    print(\"Clean Text:#\",i+1)\n",
        "    print(\"Clean Summary:\",clean_summaries[i])\n",
        "    print(\"Clean Text:\",clean_texts[i])\n",
        "    print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean Text:# 1\n",
            "Clean Summary: শাপলা চত্বরে বাসের ধাক্কায় নিহত\n",
            "Clean Text: রাজধানীর মতিঝিলের শাপলা চত্বর এলাকায় আজ বৃহস্পতিবার সন্ধ্যায় যাত্রীবাহী বাসের ধাক্কায় রঞ্জন দেবনাথ নামের এক ব্যক্তি নিহত হয়েছেন নিহত রঞ্জন দেবনাথের স্ত্রী বীণা বিশ্বাস ঢাকা মেডিকেল কলেজ হাসপাতালের স্টাফ নার্স তিনি জানান রঞ্জন দেবনাথ আফতাব গ্রুপে চাকরি করেন\n",
            "\n",
            "Clean Text:# 2\n",
            "Clean Summary: ইসলামবাগে প্লাস্টিক কারখানায় আগুন\n",
            "Clean Text: রাজধানীর পুরান ঢাকার ইসলামবাগে একটি প্লাস্টিক পণ্য তৈরির কারখানায় আগুন লেগেছে আজ বৃহস্পতিবার রাত টা মিনিটে বাগানবাড়ি এলাকায় এই অগ্নিকাণ্ডের ঘটনা ঘটে ফায়ার সার্ভিসের নিয়ন্ত্রণকক্ষে দায়িত্বরত কর্মকর্তা ফরহাদুজ্জামান প্রথম আলোকে জানান আগুন নেভাতে ফায়ার সার্ভিসের পাঁচটি ইউনিট কাজ করছে তবে আগুন লাগার কারণ এখনো জানা যায়নি\n",
            "\n",
            "Clean Text:# 3\n",
            "Clean Summary: অনুমতি না পেলেও সমাবেশ করবে বিএনপি\n",
            "Clean Text: আগামী জানুয়ারি অনুমতি না পেলেও রাজধানীতে বিএনপি সমাবেশ করবে বলে জানিয়েছেন দলের যুগ্ম মহাসচিব রুহুল কবির রিজভী আজ বৃহস্পতিবার সন্ধ্যায় জানতে চাইলে প্রথম আলোকে রিজভী জানান আমরা কর্মসূচি ঘোষণা করেছি শান্তিপূর্ণভাবে এ কর্মসূচি পালন করব\n",
            "\n",
            "Clean Text:# 4\n",
            "Clean Summary: ফকিরাপুলে আবাসিক হোটেলে খুন\n",
            "Clean Text: রাজধানীর ফকিরাপুলে আজ বৃহস্পতিবার সন্ধ্যা সাতটার দিকে একটি আবাসিক হোটেলে দুর্বৃত্তদের গুলিতে আবদুল হামিদ নামের এক ব্যক্তি খুন হয়েছেন হামিদের বাড়ি চট্টগ্রাম জেলার সন্দ্বীপে পুলিশ ও ঢাকা মেডিকেল কলেজ হাসপাতাল সূত্রে জানা যায় আবদুল হামিদ সপ্তাহ খানেক আগে মতিঝিল থানাধীন ফকিরাপুল এলাকার হোটেল মতিঝিলের নম্বর কক্ষ ভাড়া নেন\n",
            "\n",
            "Clean Text:# 5\n",
            "Clean Summary: গত বছর জন নারী নির্যাতনের শিকার\n",
            "Clean Text: গত বছর মোট হাজার জন নারী বিভিন্নভাবে নির্যাতনের শিকার হয়েছেন এর মধ্যে যৌতুকের জন্য নির্যাতনের শিকার হন জন যৌতুকের কারণে হত্যা করা হয় জনকে উত্ত্যক্তের শিকার হন জন এঁদের মধ্যে আত্মহত্যা করেন জন ফতোয়ার শিকার হয়েছেন জন\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_alT6IB-blH"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiyzDGoVyIYH"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF2VeDzvxNPM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "a55f2fcd-4986-4968-b7df-77b57c28dc58"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in df['Text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in df['Summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeP0lEQVR4nO3dfbRcVZnn8e/PIKiIJoh9xSR2gkZ6IRkRMsiMtn3HKK+2wVlKh8U0ATJGp6HVMa0GdRZMIzPQY0QYnTjRIGAjLw0qGYmtEVILWTMJEkDeaQJeTLJCwksCVhQ04Zk/zi5yUvdWblXdulWncn6ftWrVOfvsc+q5lZOnTu2za29FBGZmVg6v6HUAZmbWPU76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76BSVpSNIHinIcM9s7OOmbWelJ2qfXMXSLk34BSfoe8Bbg/0iqSvq8pGMk/V9J2yT9StJgqvtvJT0taWpaf6ekrZL+bKTj9OyPsr2epC9I2ijpt5IekTRb0hWSvpKrMyhpQ259SNLnJN0rabukZZIGJP0kHefnkialutMkhaQzJa1P5/knJf3rtP82Sd/IHfutkm6V9Ez6P3K1pIl1r/0FSfcC21McN9b9TZdJunRc37huiwg/CvgAhoAPpOXJwDPAiWQf1B9M629M2y8EbgVeDdwHnDPScfzwY7wewKHAeuDNaX0a8FbgCuAruXqDwIbc+hCwGhhI5/kW4C7gXcCr0nl9Xu6YAXwrbTsWeAH4EfAnuf3/ItV/W/q/sh/wRuA24Ot1r30PMDX93zkY2A5MTNv3Scc7qtfvbycfvtLvD/8BWBERKyLipYhYCdxJ9iEAcD7weuAOYCPwzZ5EaWW2kyy5HibplRExFBGPNbnv/4yIzRGxEfgFsCYi7o6IF4Afkn0A5F0QES9ExM/IkvQ1EbElt/+7ACJiXUSsjIgXI+Ip4GvAX9Qd67KIWB8Rv4+ITWQfDB9L244Hno6ItS29EwXnpN8f/hT4WPr6uk3SNuC9ZFcmRMQfya6oDgcWR7pMMeuWiFgHfIbsAmSLpGslvbnJ3Tfnln8/wvpr26mfmomuTU1OzwP/CBxUd6z1detXkl1kkZ6/1+Tf0Dec9Isrn7jXA9+LiIm5x/4RcRGApMnAecB3gcWS9mtwHLNxExHfj4j3kl2kBHAx2ZX4a3LV3tTFkP5bimNmRLyOLImrrk79/48fAf9K0uHAh4Crxz3KLnPSL67NwCFp+R+Bv5R0nKQJkl6VbohNkSSyq/xlwHxgE3BBg+OYjQtJh0p6f7rgeIHsivslsjbzEyUdKOlNZN8GuuUAoAo8ly6MPjfaDqlJ6Qbg+8AdEfGb8Q2x+5z0i+u/A19OTTl/BcwBvgg8RXbl/zmyf79Pkd3E+i+pWedM4ExJf15/HEl/1+W/wcpjP+Ai4GngSbJz8lyy5pFfkd00/RlwXRdj+q/AkcBzwM3AD5rc70pgJnth0w6A3PxrZraLpLcADwNviojnex1Pp/lK38wskfQK4LPAtXtjwoesH6qZWelJ2p/sHtgTZN0190pu3jEzKxE375iZlUjhm3cOOuigmDZt2m5l27dvZ//99+9NQGPQr3FD/8a+fft2Hn744acj4o29jqVZ+XO+iO+7Y2per+Jau3Zt43O+1+NAjPY46qijot6qVauGlfWDfo07on9jX7VqVQB3RgHO5WYf+XO+iO+7Y2per+La0znv5h0zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MyuRUZO+pMslbZF0f67sOkn3pMeQpHtS+TRJv89t+1Zun6Mk3SdpXZp3sn5cazMzG2fN/DjrCuAbwFW1goj4q9qypMVkQ5fWPBYRR4xwnCXAx4E1wAqysS1+0nrIZmbWrlGv9CPiNuDZkbalq/VTgGv2dAxJBwOvi4jV6YcDVwEntx6umZmNxViHYfhzYHNEPJormy7pbuB54MsR8QuyWeo35OpsSGVtm7bo5mFlQxedNJZDmvWd+v8H/j9goxlr0j+V3a/yNwFviYhnJB0F/EjSO1o9qKQFwAKAgYEBKpXKbtur1SoLZ+4ctl99vaKpVquFj7GRfo29Wq32OgSzQmk76UvaB/j3wFG1soh4EXgxLa+V9BjwdmAjMCW3+5RUNqKIWAosBZg1a1YMDg7utr1SqbD49u3D9hs6bXBYWZFUKhXq/5Z+0a+x9+MHldl4GkuXzQ8AD0fEy802kt4oaUJaPgSYATweEZuA5yUdk+4DnA7cNIbXNjOzNjTTZfMa4P8Bh0raIGl+2jSX4Tdw3wfcm7pw3gB8MiJqN4H/BvgOsA54DPfcMTPrulGbdyLi1AblZ4xQdiNwY4P6dwKHtxifmZl1kH+Ra2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmI2gwY9yBklZKejQ9T0rlSrPBrZN0r6Qjc/vMS/UflTSvF3+LWZ6TvtnIriCb3S1vEXBLRMwAbknrACeQDS44g2xI8CWQfUgA5wHvBo4Gzqt9UJj1ipO+2QgazBg3B7gyLV/Jrtnf5gBXRWY1MDHNFnccsDIino2IrcBKhn+QmHXVWCdRMSuTgTRMOMCTwEBangysz9WrzQzXqHyYRhMHjTZ5zcKZO3Zb78b8AUWcUKeIMUEx43LSN2tDRISk6ODxRpw4aLTJa86ony6xCxMJFXFCnSLGBMWMy807Zs3bnJptSM9bUvlGYGquXm1muEblZj3jpG/WvOVArQfOPHbN/rYcOD314jkGeC41A/0UOFbSpHQD99hUZtYzbt4xG0GaMW4QOEjSBrJeOBcB16fZ454ATknVVwAnks0K9zvgTICIeFbSBcAvU72/z80kZ9YTTvpmI2g0Yxwwe4S6AZzd4DiXA5d3MDSzMXHzjplZiTjpm5mViJO+mVmJjJr0G4xBcr6kjZLuSY8Tc9vOTWOQPCLpuFz58alsnaRF9a9jZmbjr5kr/SsY+afjl0TEEemxAkDSYcBc4B1pn/8laYKkCcA3ycYoOQw4NdU1M7MuGrX3TkTcJmlak8ebA1wbES8Cv5a0jmygKYB1EfE4gKRrU90HW47YzMzaNpYum+dIOh24E1iYBpSaDKzO1cmPNVI/Bsm7Gx240TgkNdVqlYUzdw7br2hjXNQr4jgczerX2KvVaq9DMCuUdpP+EuACINLzYuCsTgXVaBySmkqlwuLbtw/brxvjjoxFEcfhaFa/xt6PH1Rm46mtpB8Rm2vLkr4N/Dit7mmsEY9BYmbWY2112awNOpV8BKj17FkOzJW0n6TpZJNK3EH2M/QZkqZL2pfsZu/y9sM2M7N2jHql32AMkkFJR5A17wwBnwCIiAckXU92g3YHcHZE7EzHOYdssKkJwOUR8UDH/xozM9ujZnrvjDQGybI91L8QuHCE8hVkA1OZmVmP+Be5ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiUylonRzaxgpi26eVjZ0EUn9SASKypf6Zu1SNJ/lvSApPslXSPpVWkq0DWS1km6Lk0LSpo69LpUvkbStN5Gb2XnpG/WAkmTgU8BsyLicLLpP+cCFwOXRMTbgK3A/LTLfGBrKr8k1TPrGSd9s9btA7xa0j7Aa4BNwPuBG9L2K4GT0/KctE7aPluSuhir2W6c9M1aEBEbga8CvyFL9s8Ba4FtEbEjVdsATE7Lk4H1ad8dqf4buhmzWd6oN3IlXQ58CNiSvs4i6X8Afwn8AXgMODMitqX2yoeAR9LuqyPik2mfo4ArgFeTTZD+6YiITv4xZuNN0iSyq/fpwDbgn4DjO3DcBcACgIGBASqVCgDVavXl5ZEsnLmj4baaPe3fjtFi6oUixgTFjKuZ3jtXAN8ArsqVrQTOjYgdki4GzgW+kLY9FhFHjHCcJcDHgTVkSf944Cdtxm3WKx8Afh0RTwFI+gHwHmCipH3S1fwUYGOqvxGYCmxIzUGvB56pP2hELAWWAsyaNSsGBweBLGHXlkdyxgi9deoNndZ4/3aMFlMvFDEmKGZcozbvRMRtwLN1ZT/LfZVdTXaSNyTpYOB1EbE6Xd1fxa42T7N+8hvgGEmvSW3zs4EHgVXAR1OdecBNaXl5Widtv9XfcK2XOtFP/yzgutz6dEl3A88DX46IX5C1a27I1cm3eQ7T6KtuTbVaZeHMncP2K9rXqHpF/KrXrH6NvVqtdvR4EbFG0g3AXcAO4G6yK/SbgWslfSWVLUu7LAO+J2kd2cXT3I4GZNaiMSV9SV8iO/GvTkWbgLdExDOpDf9Hkt7R6nEbfdWtqVQqLL59+7D9Ov01ttOK+FWvWf0a+3h8UEXEecB5dcWPA0ePUPcF4GMdD8KsTW0nfUlnkN3gnV37uhoRLwIvpuW1kh4D3k7WrplvAsq3eZqZWZe01WVT0vHA54EPR8TvcuVvlDQhLR8CzAAej4hNwPOSjkntoKezq83TzMy6pJkum9cAg8BBkjaQfa09F9gPWJl+Z1Lrmvk+4O8l/RF4CfhkRNRuAv8Nu7ps/gT33DEz67pRk35EnDpC8bIRyoiIG4EbG2y7Ezi8pejMzKyj/ItcM7MScdI3MysRJ30zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MysRJ30zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MysRJ30zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MysRJ30zsxJpKulLulzSFkn358oOlLRS0qPpeVIql6TLJK2TdK+kI3P7zEv1H5U0r/N/jpmZ7UmzV/pXAMfXlS0CbomIGcAtaR3gBGBGeiwAlkD2IQGcB7wbOBo4r/ZBYWZm3dFU0o+I24Bn64rnAFem5SuBk3PlV0VmNTBR0sHAccDKiHg2IrYCKxn+QWJmZuNonzHsOxARm9Lyk8BAWp4MrM/V25DKGpUPI2kB2bcEBgYGqFQqu22vVqssnLlz2H719YqmWq0WPsZG+jX2arXa6xDMCmUsSf9lERGSohPHSsdbCiwFmDVrVgwODu62vVKpsPj27cP2GzptcFhZkVQqFer/ln7Rr7GPxweVpInAd4DDgQDOAh4BrgOmAUPAKRGxVZKAS4ETgd8BZ0TEXR0PyqxJY+m9szk125Cet6TyjcDUXL0pqaxRuVm/uRT454j4M+CdwEO0eI/LrFfGkvSXA7UeOPOAm3Llp6dePMcAz6VmoJ8Cx0qalG7gHpvKzPqGpNcD7wOWAUTEHyJiG63f4zLriaaadyRdAwwCB0naQNYL5yLgeknzgSeAU1L1FWRfZdeRfZ09EyAinpV0AfDLVO/vI6L+5rBZ0U0HngK+K+mdwFrg07R+j2tTrqzhfazR7qUsnLlj1IA73cRVxPs7RYwJihlXU0k/Ik5tsGn2CHUDOLvBcS4HLm86OrPi2Qc4EvjbiFgj6VJ2NeUA7d3janQfa7R7KWcsunnUY3f6XlcR7+8UMSYoZlz+Ra5ZazYAGyJiTVq/gexDoNV7XGY94aRv1oKIeBJYL+nQVDQbeJDW73GZ9URHumyalczfAldL2hd4nOy+1Sto4R6XWa846Zu1KCLuAWaNsKmle1ytmtZE+73ZaNy8Y2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiXSdtKXdKike3KP5yV9RtL5kjbmyk/M7XOupHWSHpF0XGf+BDMza1bbM2dFxCPAEQCSJpBN9vxDsungLomIr+brSzoMmAu8A3gz8HNJb4+Ine3GYGZmrelU885s4LGIeGIPdeYA10bEixHxa7I5Q4/u0OubmVkTOjVH7lzgmtz6OZJOB+4EFkbEVmAysDpXZ0MqG0bSAmABwMDAAJVKZbft1WqVhTOHf0Gor1c01Wq18DE20q+xV6vVXodgVihjTvqS9gU+DJybipYAFwCRnhcDZ7VyzIhYCiwFmDVrVgwODu62vVKpsPj27cP2GzptcFhZkVQqFer/ln7Rr7H34weV2XjqRPPOCcBdEbEZICI2R8TOiHgJ+Da7mnA2AlNz+01JZWZm1iWdSPqnkmvakXRwbttHgPvT8nJgrqT9JE0HZgB3dOD1zcysSWNq3pG0P/BB4BO54n+QdARZ885QbVtEPCDpeuBBYAdwtnvumJl115iSfkRsB95QV/bXe6h/IXDhWF7TzMza51/kmpmViJO+mVmJOOmbmZWIk76ZWYk46Zu1SNIESXdL+nFany5pTRpM8Lr0g0VS9+TrUvkaSdN6GbcZOOmbtePTwEO59YvJBhl8G7AVmJ/K5wNbU/klqZ5ZTznpm7VA0hTgJOA7aV3A+4EbUpUrgZPT8py0Tto+O9U365lODbhmVhZfBz4PHJDW3wBsi4gdaT0/kOBkYD1AROyQ9Fyq/3T9QRsNMpgf6G7hzB31uzWl0+MPFXHwvSLGBMWMy0nfrEmSPgRsiYi1kgY7eexGgwzmB7o7Y9HNbR270wMRFnHwvSLGBMWMy0nfrHnvAT6cZoN7FfA64FJgoqR90tV+fiDB2iCDGyTtA7weeKb7YZvt4jZ9syZFxLkRMSUippHNIXFrRJwGrAI+mqrNA25Ky8vTOmn7rRERXQzZbBgnfbOx+wLwWUnryNrsl6XyZcAbUvlngUU9is/sZW7eMWtDRFSASlp+nBGm/oyIF4CPdTUws1H4St/MrESc9M3MSsRJ38ysRJz0zcxKxEnfzKxEnPTNzErESd/MrESc9M3MSmTMSV/SkKT7JN0j6c5UdqCklZIeTc+TUrkkXZYmlbhX0pFjfX0zM2tep670/11EHBERs9L6IuCWiJgB3MKun5+fAMxIjwXAkg69vpmZNWG8mnfyk0fUTypxVWRWk41OePA4xWBmZnU6MfZOAD+TFMD/TuOCD0TEprT9SWAgLb88qURSm3BiU66s4YQSNdVqlYUzdw4LpGiTFdQr4oQKzerX2KvVaq9DMCuUTiT990bERkl/AqyU9HB+Y0RE+kBoWqMJJWoqlQqLb98+fMf7di8buuikVl523BVxQoVm9Wvs/fhBZTaexty8ExEb0/MW4Idkow1urjXbpOctqXptUoma/IQTZmY2zsaU9CXtL+mA2jJwLHA/u08eUT+pxOmpF88xwHO5ZiAzMxtnY23eGQB+KKl2rO9HxD9L+iVwvaT5wBPAKan+CuBEYB3wO+DMMb6+mZm1YExJP00e8c4Ryp8BZo9QHsDZY3lNMzNrn3+Ra2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb9YCSVMlrZL0oKQHJH06lR8oaaWkR9PzpFQuSZdJWifpXklH9vYvsLJz0jdrzQ5gYUQcBhwDnC3pMGARcEtEzABuSesAJwAz0mMBsKT7IZvt4qRv1oKI2BQRd6Xl3wIPkc3zPAe4MlW7Ejg5Lc8BrorMamBibVY5s17oxBy5ZqUkaRrwLmANMJCbBe5JsgmGIPtAWJ/bbUMq223GOEkLyL4JMDAw8PLcvvkJ6RfO3NFWnJ2eJzgfU1EUMSYoZlxO+mZtkPRa4EbgMxHxfJo9DsgmC5IUrRwvIpYCSwFmzZoVtUno8xPSn7Ho5rZiHTptsK39GsnHVBRFjAmKGZebd8xaJOmVZAn/6oj4QSreXGu2Sc9bUvlGYGpu9ympzKwnnPTNWqDskn4Z8FBEfC23aTkwLy3PA27KlZ+eevEcAzyXawYy6zo375i15j3AXwP3SbonlX0RuAi4XtJ84AnglLRtBXAisA74HXBmd8M1213bSV/SVOAqshtWASyNiEslnQ98HHgqVf1iRKxI+5wLzAd2Ap+KiJ+OIXazrouI2wE12Dx7hPoBnD2uQZm1YCxX+rX+yndJOgBYK2ll2nZJRHw1Xzn1ZZ4LvAN4M/BzSW+PiJ1jiMHMzFrQdpv+HvorNzIHuDYiXoyIX5N93T263dc3M7PWdaRNv66/8nuAcySdDtxJ9m1gK9kHwurcbrX+yiMdb8Q+yzXVapWFM0f/glC0/rFF7LPbrH6NvVqt9joEs0IZc9Ifob/yEuACsnb+C4DFwFmtHLNRn+WaSqXC4tu3j3qcTvdPHqsi9tltVr/G3o8fVGbjaUxdNkfqrxwRmyNiZ0S8BHybXU047q9sZtZjbSf9Rv2V68YV+Qhwf1peDsyVtJ+k6WQDUN3R7uubmVnrxtK806i/8qmSjiBr3hkCPgEQEQ9Iuh54kKznz9nuuWNm1l1tJ/099FdesYd9LgQubPc1zax10+rG7Bm66KQeRWJF4GEYzMxKxEnfzKxEnPTNzErESd/MrET26lE2fQPLzGx3vtI3MysRJ30zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MysRJ30zsxLZq3+cVa/+x1rgH2yZWbmUKumbmX+pXnZu3jEzK5HSX+n7qsfMyqT0Sd+s7Hyvq1zcvGNmViK+0q/jqx4z25s56TdhpA+CPH8omFm/6HrzjqTjJT0iaZ2kRd1+fbNu8zlvRdLVK31JE4BvAh8ENgC/lLQ8Ih7sZhyd5iYha2RvPeetf3W7eedoYF1EPA4g6VpgDrDX/QcY6YNg4cwdnLGHpqJmPija6WI6WvNUs8extvTlOb+nc2a08zjP51XxdDvpTwbW59Y3AO+uryRpAbAgrVYlPVJX5SDg6XGJcBx9apS4dXHrx2xnnzaP05fvOVncf9rD1x/rOV+493208zivU+dnEwr3PiW9iqvhOV/IG7kRsRRY2mi7pDsjYlYXQ+qIfo0b+jf2FPe0XscxmkbnfBHfd8fUvCLG1e0buRuBqbn1KanMbG/lc94KpdtJ/5fADEnTJe0LzAWWdzkGs27yOW+F0tXmnYjYIekc4KfABODyiHigjUM1bPopuH6NG/o39p7G3YFzvojvu2NqXuHiUkT0OgYzM+sSj71jZlYiTvpmZiXSd0m/6D9plzQk6T5J90i6M5UdKGmlpEfT86RULkmXpb/lXklHdjHOyyVtkXR/rqzlOCXNS/UflTSvR3GfL2ljes/vkXRibtu5Ke5HJB2XKy/6edTz+CRNlbRK0oOSHpD06VQ+4nnS5dgmSLpb0o/T+nRJa9L7dV26ad7tmCZKukHSw5IekvRvivBeDRMRffMguxH2GHAIsC/wK+CwXsdVF+MQcFBd2T8Ai9LyIuDitHwi8BNAwDHAmi7G+T7gSOD+duMEDgQeT8+T0vKkHsR9PvB3I9Q9LJ0j+wHT07kzoejnUVHiAw4GjkzLBwD/kt7TEc+TLsf2WeD7wI/T+vXA3LT8LeA/9SCmK4H/mJb3BSYW4b2qf/Tblf7LP2mPiD8AtZ+0F90cshOC9HxyrvyqyKwGJko6uBsBRcRtwLNjjPM4YGVEPBsRW4GVwPE9iLuROcC1EfFiRPwaWEd2DhX9PCpEfBGxKSLuSsu/BR4i+4Vxo/OkKyRNAU4CvpPWBbwfuKGHMb2e7IJkGUBE/CEittHj92ok/Zb0R/pJ++QexdJIAD+TtDb9tB5gICI2peUngYG0XLS/p9U4ixT/Oanp6fLcV+h+iHskhYtP0jTgXcAaGp8n3fJ14PPAS2n9DcC2iNiR1nvxfk0HngK+m5qdviNpf3r/Xg3Tb0m/H7w3Io4ETgDOlvS+/MbIvucVvp9sv8SZLAHeChwBbAIW9zacvYuk1wI3Ap+JiOfz27p9nkj6ELAlItZ26zWbtA9Zs+OSiHgXsJ2sOedlRfk/1W9Jv/A/aY+Ijel5C/BDsq/qm2vNNul5S6petL+n1TgLEX9EbI6InRHxEvBtsvecPcRXiLj3oDDxSXolWcK/OiJ+kIobnSfd8B7gw5KGyJq93g9cStbkWPuxaS/erw3AhohYk9ZvIPsQ6OV7NaJ+S/qF/km7pP0lHVBbBo4F7ieLsdazZR5wU1peDpyeesccAzyX+yrYC63G+VPgWEmTUpPKsamsq+rug3yE7D2HLO65kvaTNB2YAdxBwc8jChJfaitfBjwUEV/LbWp0noy7iDg3IqZENojeXODWiDgNWAV8tBcxpbieBNZLOjQVzSYbPrtn71VDvb6T3OqDrCfJv5D1bvhSr+Opi+0Qsp4WvwIeqMVH1uZ4C/Ao8HPgwFQusgk2HgPuA2Z1MdZryJpC/kh2lTK/nTiBs8hukK4DzuxR3N9Lcd1L9p/s4Fz9L6W4HwFO6IfzqCjxAe8la464F7gnPU5sdJ70IL5BdvXeOYTsA30d8E/Afj2I5wjgzvR+/YisR1sh3qv8w8MwmJmVSL8175iZ2Rg46ZuZlYiTvplZiTjpm5mViJO+mVmJOOmbmZWIk76ZWYn8f8V8TrbiZOHUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glTQpC5sKVkL"
      },
      "source": [
        "\n",
        "def count_words(count_dict, text):\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G00eT0eKY8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9775bf40-8f07-4e17-a273-8949e45028da"
      },
      "source": [
        "word_counts = {}\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 31210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz0EYhAGKb-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188b2a94-2664-458e-9aca-3b209a1ff8a3"
      },
      "source": [
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "with open('/content/gdrive/My Drive/Text summarization/bn_w2v_model.text', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 497405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzPzSWpeKmUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351c69ab-d458-440b-8925-c8185b10deb1"
      },
      "source": [
        "embeddings_index[\"জাতীয়\"]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.05410e-02,  6.26330e-02, -5.53490e-02, -2.54590e-02,\n",
              "        6.18490e-02,  2.27295e-01, -1.29583e-01,  4.57040e-02,\n",
              "        3.60720e-02,  8.10060e-02, -1.31711e-01, -1.11894e-01,\n",
              "        1.43287e-01, -2.13770e-02, -3.22964e-01,  2.46480e-02,\n",
              "        8.61500e-02, -5.79400e-02,  8.86090e-02,  8.51600e-02,\n",
              "        6.89690e-02, -1.11190e-02,  3.45341e-01, -8.32900e-02,\n",
              "       -2.80570e-02, -9.46200e-03,  3.60380e-02,  2.17625e-01,\n",
              "       -6.20290e-02, -1.26047e-01, -1.38668e-01,  2.20440e-02,\n",
              "        8.01260e-02,  1.38463e-01, -1.07327e-01, -1.64391e-01,\n",
              "        8.87740e-02, -5.59450e-02,  8.10050e-02,  3.00634e-01,\n",
              "        2.22209e-01,  7.90780e-02,  1.71066e-01, -1.06080e-02,\n",
              "        6.86540e-02,  2.15358e-01,  8.28820e-02, -5.63270e-02,\n",
              "       -2.28050e-02, -8.81600e-02, -2.08755e-01, -4.14620e-02,\n",
              "       -3.19320e-02,  7.80410e-02,  3.07990e-02,  7.03770e-02,\n",
              "       -6.82880e-02,  3.51410e-02, -6.10490e-02, -2.59885e-01,\n",
              "       -2.71876e-01,  1.55580e-02,  9.79880e-02,  1.09827e-01,\n",
              "        3.10850e-02, -4.21980e-02, -6.00570e-02,  1.46701e-01,\n",
              "       -1.91746e-01,  1.75303e-01, -7.56020e-02,  2.73142e-01,\n",
              "        5.31250e-02,  1.74924e-01, -1.20288e-01,  1.17599e-01,\n",
              "        3.35180e-02,  4.74650e-02,  1.23473e-01,  4.00830e-02,\n",
              "        1.07855e-01, -3.72010e-02, -1.30610e-01, -5.69700e-02,\n",
              "        1.78766e-01, -1.18834e-01, -1.11804e-01, -1.22200e-02,\n",
              "        1.42180e-01,  9.35900e-03,  3.61640e-02, -3.87350e-02,\n",
              "       -2.94666e-01, -1.74020e-01,  1.32890e-02,  7.07660e-02,\n",
              "        2.02473e-01, -1.25324e-01,  1.27112e-01, -1.18834e-01,\n",
              "        3.12493e-01, -5.04280e-02,  3.71170e-02, -1.03934e-01,\n",
              "       -3.13890e-02,  8.45130e-02,  2.45800e-03,  2.56844e-01,\n",
              "       -2.09111e-01, -1.48620e-01, -1.66505e-01,  1.35450e-02,\n",
              "       -1.42589e-01,  2.80583e-01,  1.19107e-01,  3.24420e-02,\n",
              "        1.06153e-01, -3.06310e-02, -4.17870e-02,  9.33420e-02,\n",
              "       -5.14190e-02,  2.50475e-01, -3.07500e-01, -2.67630e-02,\n",
              "       -9.77940e-02,  8.74770e-02,  5.93560e-02, -4.14846e-01,\n",
              "       -3.77010e-02, -1.94183e-01, -8.06550e-02, -1.49484e-01,\n",
              "       -5.99760e-02, -1.98677e-01,  1.08279e-01,  1.26575e-01,\n",
              "       -1.48403e-01, -1.92593e-01, -2.64770e-02, -5.89970e-02,\n",
              "       -2.20963e-01, -1.79310e-01, -1.73251e-01, -1.62942e-01,\n",
              "       -1.58848e-01, -1.25680e-02, -6.42490e-02, -1.65488e-01,\n",
              "       -1.30516e-01,  9.00450e-02, -2.54740e-02,  2.27880e-02,\n",
              "        1.48193e-01, -3.18616e-01, -7.00410e-02,  1.59200e-02,\n",
              "        3.62110e-02, -1.13170e-02, -1.50700e-01, -2.17230e-02,\n",
              "       -2.09670e-02, -2.18406e-01,  1.47709e-01,  6.66850e-02,\n",
              "        9.05600e-03,  3.04180e-02,  6.28840e-02, -1.14266e-01,\n",
              "        9.80370e-02, -7.74700e-03,  4.65810e-02, -1.12580e-02,\n",
              "       -7.71270e-02,  1.31204e-01,  2.09780e-02,  6.57070e-02,\n",
              "       -1.55410e-01, -4.58900e-03, -1.37301e-01, -9.89130e-02,\n",
              "       -2.30561e-01, -5.26000e-04,  1.77451e-01, -3.97680e-02,\n",
              "       -1.58343e-01, -1.09670e-02,  1.09941e-01, -1.70477e-01,\n",
              "        1.33490e-02, -1.32206e-01, -4.55600e-03,  6.46380e-02,\n",
              "       -1.74557e-01,  3.79300e-02, -1.20047e-01,  3.25857e-01,\n",
              "        3.13838e-01,  5.52500e-02, -4.77460e-02, -2.39500e-03,\n",
              "        3.66990e-02,  2.05000e-03,  2.66273e-01,  2.63368e-01,\n",
              "        1.15523e-01,  2.41999e-01, -3.89350e-02, -1.38928e-01,\n",
              "       -6.94030e-02, -8.07760e-02,  1.88154e-01,  6.71350e-02,\n",
              "       -4.08380e-02, -1.09416e-01,  1.71999e-01, -2.27683e-01,\n",
              "        1.04273e-01, -6.59860e-02, -9.73010e-02,  1.59745e-01,\n",
              "       -1.78655e-01,  2.51110e-02, -1.27140e-01,  5.34650e-02,\n",
              "       -7.76700e-02, -1.70201e-01,  2.76461e-01, -1.08624e-01,\n",
              "       -6.79660e-02, -7.61260e-02,  1.14872e-01,  1.47351e-01,\n",
              "       -9.48110e-02, -1.96020e-02,  8.20000e-05,  1.15261e-01,\n",
              "       -6.66160e-02, -5.57430e-02, -2.36985e-01, -1.59865e-01,\n",
              "       -5.38230e-02,  2.28222e-01,  1.82748e-01, -1.23190e-01,\n",
              "       -1.63350e-02,  2.61400e-03, -1.60659e-01, -6.50890e-02,\n",
              "       -9.16620e-02,  1.29832e-01, -1.02635e-01, -4.66910e-02,\n",
              "        8.80010e-02,  3.07060e-02, -2.85478e-01,  3.27850e-02,\n",
              "       -8.60080e-02, -1.93383e-01,  9.75630e-02,  1.69290e-01,\n",
              "       -4.00690e-02,  3.21337e-01, -1.98800e-02, -3.09000e-04,\n",
              "        2.05522e-01, -1.80800e-01, -1.43985e-01,  2.83691e-01,\n",
              "       -1.98185e-01,  1.04180e-02,  3.79380e-02,  1.92834e-01,\n",
              "        9.73550e-02, -3.01850e-02,  1.31395e-01,  4.16600e-03,\n",
              "       -1.73148e-01, -1.60143e-01,  8.19950e-02,  4.10143e-01,\n",
              "       -1.05157e-01, -6.05810e-02,  1.23674e-01, -1.16269e-01,\n",
              "        1.81575e-01, -6.36940e-02, -1.18455e-01,  1.38480e-01,\n",
              "        3.29950e-02, -1.52042e-01,  2.24914e-01, -1.00939e-01,\n",
              "       -7.34060e-02,  6.40110e-02, -8.76890e-02,  1.23521e-01,\n",
              "        2.53300e-02,  5.27080e-02, -1.71612e-01, -1.17880e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJVUURRSLCRr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c943edd-7c57-4f40-84e8-6b1223ee8e34"
      },
      "source": [
        "missing_words = 0\n",
        "threshold = 5\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),2)*100            \n",
        "print(\"Number of words missing from cc-bn:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from cc-bn: 52\n",
            "Percent of words that are missing from vocabulary: 0.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N5r7Xs-LLuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabb9595-389b-43d3-a77e-76b94e2c9395"
      },
      "source": [
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 31210\n",
            "Number of words we will use: 26902\n",
            "Percent of words we will use: 86.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSfXUV-rLQ4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6246e6ef-9c3d-4fbb-bcb0-5d3cf23631ec"
      },
      "source": [
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1581oQyPLYyE"
      },
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbRWPeAhLc-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b22b332-a0ac-43d6-af14-aa242786a026"
      },
      "source": [
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 238637\n",
            "Total number of UNKs in headlines: 5353\n",
            "Percent of words that are UNK: 2.2399999999999998%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUcSwdiMLf6b"
      },
      "source": [
        "\n",
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi66jOHpLoPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebabedc-64b3-4953-f62a-0c5fe8089b7a"
      },
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "            counts\n",
            "count  2731.000000\n",
            "mean      6.292201\n",
            "std       2.822517\n",
            "min       2.000000\n",
            "25%       5.000000\n",
            "50%       6.000000\n",
            "75%       7.000000\n",
            "max      73.000000\n",
            "\n",
            "Texts:\n",
            "            counts\n",
            "count  2731.000000\n",
            "mean     82.088612\n",
            "std     105.469327\n",
            "min      10.000000\n",
            "25%      38.000000\n",
            "50%      49.000000\n",
            "75%      73.000000\n",
            "max    1486.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMb2zQW-Lqfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def8ad4b-615f-4451-bfa8-e99333772dcf"
      },
      "source": [
        "\n",
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "175.0\n",
            "265.5\n",
            "509.39999999999964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIUzQrqvLy7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86380cfb-402a-46d5-f110-a220bc2101bf"
      },
      "source": [
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.0\n",
            "11.0\n",
            "15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmWWvGDgL1NO"
      },
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4w7WB-IL7_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f66419-ba18-4d6c-8cf7-a8059fa935f5"
      },
      "source": [
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 83\n",
        "max_summary_length =13\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1466\n",
            "1466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcxX_GieMEYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503eb9f7-e48f-455d-96df-e1fd7299d80b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnvoicwxMQFQ"
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2M8rFZVMVBY"
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WjADnpAMYXq"
      },
      "source": [
        "\n",
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fctiY5DAMhJc"
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYk-NERBMkWa"
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr5bSoODMnGJ"
      },
      "source": [
        "\n",
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits\n",
        "\n",
        "     "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK6P22_NMze5"
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-A6_Bp6NEuj"
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bQJdbacNHdd"
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 2\n",
        "rnn_size = 256\n",
        "num_layers = 3\n",
        "learning_rate = 0.001\n",
        "keep_probability = 0.70"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MdyVlU4NLdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b319ad-c592-4c3b-8c48-3d28866ad9b4"
      },
      "source": [
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-32-2bbaec5f7e3c>:8: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-32-2bbaec5f7e3c>:21: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f7ace80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f7ace80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f7ace80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f7ace80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3bba9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3bba9390>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3bba9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3bba9390>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3ba695f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3ba695f8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3ba695f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3ba695f8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3f793518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b11c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b11c390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b11c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b11c390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fec3b0dbac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fec3b153b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3baf7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b0dbe48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fec3b954588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huoQ3TqnNV_t"
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CvAb38EO6IB"
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajv9oVgpO8Jl"
      },
      "source": [
        "\n",
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPY5f_tVO_YR"
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npo0XTHcPB-V"
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvJRfps-PErp"
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvnuW9QEPeAn"
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQmV_zqiPign"
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKqVTFx8Pk4N"
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux4qPtGuP6v8"
      },
      "source": [
        "# **This is the gap**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsqObe8QPnrO"
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 2\n",
        "rnn_size = 256\n",
        "num_layers = 3\n",
        "learning_rate = 0.001\n",
        "keep_probability = 0.70"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACbHJ539Prl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b4a9dd-cea9-4e1c-8eae-66f9ffc7ada8"
      },
      "source": [
        "\n",
        "start = 12\n",
        "end = start + 83\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 14\n",
            "The longest text length: 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY_AqZrfQHxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94dc78a-1da2-44c7-b05a-2fa9fc4bb464"
      },
      "source": [
        "\n",
        "# Train the Model\n",
        "learning_rate_decay = 0.90\n",
        "min_learning_rate = 0.001\n",
        "display_step = 2# Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 3 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 1 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "checkpoint = \"./model1.ckpt\"  #300k sentence\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    # loader.restore(sess, checkpoint)\n",
        "    #sess.run(tf.local_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "                \n",
        "                #saver = tf.train.Saver() \n",
        "                #saver.save(sess, checkpoint)\n",
        "                \n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "              \n",
        "                  \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1/30 Batch    2/41 - Loss: 15.240, Seconds: 0.92\n",
            "Epoch   1/30 Batch    4/41 - Loss:  9.846, Seconds: 0.83\n",
            "Epoch   1/30 Batch    6/41 - Loss:  8.717, Seconds: 0.60\n",
            "Epoch   1/30 Batch    8/41 - Loss:  8.526, Seconds: 1.34\n",
            "Epoch   1/30 Batch   10/41 - Loss:  8.991, Seconds: 0.75\n",
            "Epoch   1/30 Batch   12/41 - Loss:  8.885, Seconds: 0.90\n",
            "Epoch   1/30 Batch   14/41 - Loss:  8.876, Seconds: 1.00\n",
            "Epoch   1/30 Batch   16/41 - Loss:  9.674, Seconds: 0.68\n",
            "Epoch   1/30 Batch   18/41 - Loss: 10.219, Seconds: 0.67\n",
            "Epoch   1/30 Batch   20/41 - Loss:  9.348, Seconds: 1.00\n",
            "Epoch   1/30 Batch   22/41 - Loss:  8.646, Seconds: 0.92\n",
            "Epoch   1/30 Batch   24/41 - Loss:  8.311, Seconds: 0.85\n",
            "Epoch   1/30 Batch   26/41 - Loss:  9.837, Seconds: 0.69\n",
            "Epoch   1/30 Batch   28/41 - Loss:  8.101, Seconds: 0.91\n",
            "Epoch   1/30 Batch   30/41 - Loss:  8.404, Seconds: 0.77\n",
            "Epoch   1/30 Batch   32/41 - Loss:  9.675, Seconds: 0.70\n",
            "Epoch   1/30 Batch   34/41 - Loss:  8.878, Seconds: 0.86\n",
            "Epoch   1/30 Batch   36/41 - Loss:  9.176, Seconds: 0.80\n",
            "Epoch   1/30 Batch   38/41 - Loss:  9.661, Seconds: 0.92\n",
            "Epoch   1/30 Batch   40/41 - Loss:  9.656, Seconds: 0.94\n",
            "Average loss for this update: 9.433\n",
            "New Record!\n",
            "Epoch   2/30 Batch    2/41 - Loss:  8.946, Seconds: 0.88\n",
            "Epoch   2/30 Batch    4/41 - Loss:  5.850, Seconds: 0.82\n",
            "Epoch   2/30 Batch    6/41 - Loss:  4.154, Seconds: 0.58\n",
            "Epoch   2/30 Batch    8/41 - Loss:  4.799, Seconds: 1.33\n",
            "Epoch   2/30 Batch   10/41 - Loss:  5.676, Seconds: 0.76\n",
            "Epoch   2/30 Batch   12/41 - Loss:  5.323, Seconds: 0.98\n",
            "Epoch   2/30 Batch   14/41 - Loss:  4.963, Seconds: 1.03\n",
            "Epoch   2/30 Batch   16/41 - Loss:  5.818, Seconds: 0.67\n",
            "Epoch   2/30 Batch   18/41 - Loss:  6.245, Seconds: 0.68\n",
            "Epoch   2/30 Batch   20/41 - Loss:  6.336, Seconds: 1.04\n",
            "Epoch   2/30 Batch   22/41 - Loss:  5.400, Seconds: 0.96\n",
            "Epoch   2/30 Batch   24/41 - Loss:  5.353, Seconds: 0.84\n",
            "Epoch   2/30 Batch   26/41 - Loss:  6.361, Seconds: 0.71\n",
            "Epoch   2/30 Batch   28/41 - Loss:  5.627, Seconds: 0.94\n",
            "Epoch   2/30 Batch   30/41 - Loss:  5.782, Seconds: 0.80\n",
            "Epoch   2/30 Batch   32/41 - Loss:  6.064, Seconds: 0.74\n",
            "Epoch   2/30 Batch   34/41 - Loss:  6.189, Seconds: 0.85\n",
            "Epoch   2/30 Batch   36/41 - Loss:  6.279, Seconds: 0.80\n",
            "Epoch   2/30 Batch   38/41 - Loss:  6.690, Seconds: 0.97\n",
            "Epoch   2/30 Batch   40/41 - Loss:  6.609, Seconds: 0.96\n",
            "Average loss for this update: 5.923\n",
            "New Record!\n",
            "Epoch   3/30 Batch    2/41 - Loss:  8.176, Seconds: 0.91\n",
            "Epoch   3/30 Batch    4/41 - Loss:  5.471, Seconds: 0.84\n",
            "Epoch   3/30 Batch    6/41 - Loss:  3.976, Seconds: 0.59\n",
            "Epoch   3/30 Batch    8/41 - Loss:  4.307, Seconds: 1.31\n",
            "Epoch   3/30 Batch   10/41 - Loss:  4.562, Seconds: 0.77\n",
            "Epoch   3/30 Batch   12/41 - Loss:  4.553, Seconds: 0.94\n",
            "Epoch   3/30 Batch   14/41 - Loss:  4.411, Seconds: 1.03\n",
            "Epoch   3/30 Batch   16/41 - Loss:  4.827, Seconds: 0.68\n",
            "Epoch   3/30 Batch   18/41 - Loss:  5.281, Seconds: 0.68\n",
            "Epoch   3/30 Batch   20/41 - Loss:  5.535, Seconds: 1.08\n",
            "Epoch   3/30 Batch   22/41 - Loss:  4.587, Seconds: 0.96\n",
            "Epoch   3/30 Batch   24/41 - Loss:  4.505, Seconds: 0.89\n",
            "Epoch   3/30 Batch   26/41 - Loss:  5.235, Seconds: 0.71\n",
            "Epoch   3/30 Batch   28/41 - Loss:  4.967, Seconds: 0.97\n",
            "Epoch   3/30 Batch   30/41 - Loss:  4.523, Seconds: 0.81\n",
            "Epoch   3/30 Batch   32/41 - Loss:  4.538, Seconds: 0.75\n",
            "Epoch   3/30 Batch   34/41 - Loss:  4.810, Seconds: 0.91\n",
            "Epoch   3/30 Batch   36/41 - Loss:  4.587, Seconds: 0.83\n",
            "Epoch   3/30 Batch   38/41 - Loss:  4.945, Seconds: 1.00\n",
            "Epoch   3/30 Batch   40/41 - Loss:  4.727, Seconds: 0.97\n",
            "Average loss for this update: 4.926\n",
            "New Record!\n",
            "Epoch   4/30 Batch    2/41 - Loss:  6.989, Seconds: 0.93\n",
            "Epoch   4/30 Batch    4/41 - Loss:  4.503, Seconds: 0.85\n",
            "Epoch   4/30 Batch    6/41 - Loss:  3.471, Seconds: 0.60\n",
            "Epoch   4/30 Batch    8/41 - Loss:  4.026, Seconds: 1.35\n",
            "Epoch   4/30 Batch   10/41 - Loss:  3.913, Seconds: 0.83\n",
            "Epoch   4/30 Batch   12/41 - Loss:  3.906, Seconds: 0.96\n",
            "Epoch   4/30 Batch   14/41 - Loss:  4.012, Seconds: 1.05\n",
            "Epoch   4/30 Batch   16/41 - Loss:  4.093, Seconds: 0.72\n",
            "Epoch   4/30 Batch   18/41 - Loss:  3.629, Seconds: 0.71\n",
            "Epoch   4/30 Batch   20/41 - Loss:  4.666, Seconds: 1.06\n",
            "Epoch   4/30 Batch   22/41 - Loss:  3.624, Seconds: 0.98\n",
            "Epoch   4/30 Batch   24/41 - Loss:  3.482, Seconds: 0.89\n",
            "Epoch   4/30 Batch   26/41 - Loss:  4.010, Seconds: 0.74\n",
            "Epoch   4/30 Batch   28/41 - Loss:  4.566, Seconds: 1.01\n",
            "Epoch   4/30 Batch   30/41 - Loss:  3.762, Seconds: 0.87\n",
            "Epoch   4/30 Batch   32/41 - Loss:  3.712, Seconds: 0.73\n",
            "Epoch   4/30 Batch   34/41 - Loss:  3.679, Seconds: 0.92\n",
            "Epoch   4/30 Batch   36/41 - Loss:  3.542, Seconds: 0.81\n",
            "Epoch   4/30 Batch   38/41 - Loss:  4.321, Seconds: 1.01\n",
            "Epoch   4/30 Batch   40/41 - Loss:  3.123, Seconds: 1.01\n",
            "Average loss for this update: 4.051\n",
            "New Record!\n",
            "Epoch   5/30 Batch    2/41 - Loss:  6.794, Seconds: 0.94\n",
            "Epoch   5/30 Batch    4/41 - Loss:  3.585, Seconds: 0.86\n",
            "Epoch   5/30 Batch    6/41 - Loss:  2.384, Seconds: 0.65\n",
            "Epoch   5/30 Batch    8/41 - Loss:  3.768, Seconds: 1.39\n",
            "Epoch   5/30 Batch   10/41 - Loss:  3.213, Seconds: 0.83\n",
            "Epoch   5/30 Batch   12/41 - Loss:  3.235, Seconds: 1.03\n",
            "Epoch   5/30 Batch   14/41 - Loss:  3.770, Seconds: 1.05\n",
            "Epoch   5/30 Batch   16/41 - Loss:  3.970, Seconds: 0.72\n",
            "Epoch   5/30 Batch   18/41 - Loss:  2.604, Seconds: 0.73\n",
            "Epoch   5/30 Batch   20/41 - Loss:  4.305, Seconds: 1.07\n",
            "Epoch   5/30 Batch   22/41 - Loss:  3.002, Seconds: 1.00\n",
            "Epoch   5/30 Batch   24/41 - Loss:  3.804, Seconds: 0.92\n",
            "Epoch   5/30 Batch   26/41 - Loss:  2.917, Seconds: 0.73\n",
            "Epoch   5/30 Batch   28/41 - Loss:  4.033, Seconds: 1.00\n",
            "Epoch   5/30 Batch   30/41 - Loss:  2.908, Seconds: 0.84\n",
            "Epoch   5/30 Batch   32/41 - Loss:  2.397, Seconds: 0.75\n",
            "Epoch   5/30 Batch   34/41 - Loss:  3.503, Seconds: 0.91\n",
            "Epoch   5/30 Batch   36/41 - Loss:  3.208, Seconds: 0.85\n",
            "Epoch   5/30 Batch   38/41 - Loss:  2.509, Seconds: 1.02\n",
            "Epoch   5/30 Batch   40/41 - Loss:  2.405, Seconds: 1.02\n",
            "Average loss for this update: 3.416\n",
            "New Record!\n",
            "Epoch   6/30 Batch    2/41 - Loss:  5.599, Seconds: 0.91\n",
            "Epoch   6/30 Batch    4/41 - Loss:  3.056, Seconds: 0.83\n",
            "Epoch   6/30 Batch    6/41 - Loss:  3.314, Seconds: 0.59\n",
            "Epoch   6/30 Batch    8/41 - Loss:  2.657, Seconds: 1.39\n",
            "Epoch   6/30 Batch   10/41 - Loss:  2.851, Seconds: 0.83\n",
            "Epoch   6/30 Batch   12/41 - Loss:  2.513, Seconds: 0.99\n",
            "Epoch   6/30 Batch   14/41 - Loss:  3.386, Seconds: 1.05\n",
            "Epoch   6/30 Batch   16/41 - Loss:  3.664, Seconds: 0.71\n",
            "Epoch   6/30 Batch   18/41 - Loss:  1.960, Seconds: 0.69\n",
            "Epoch   6/30 Batch   20/41 - Loss:  3.903, Seconds: 1.03\n",
            "Epoch   6/30 Batch   22/41 - Loss:  2.829, Seconds: 1.02\n",
            "Epoch   6/30 Batch   24/41 - Loss:  3.960, Seconds: 0.88\n",
            "Epoch   6/30 Batch   26/41 - Loss:  3.241, Seconds: 0.67\n",
            "Epoch   6/30 Batch   28/41 - Loss:  3.592, Seconds: 1.08\n",
            "Epoch   6/30 Batch   30/41 - Loss:  2.670, Seconds: 0.93\n",
            "Epoch   6/30 Batch   32/41 - Loss:  1.491, Seconds: 0.80\n",
            "Epoch   6/30 Batch   34/41 - Loss:  2.550, Seconds: 1.00\n",
            "Epoch   6/30 Batch   36/41 - Loss:  2.311, Seconds: 0.94\n",
            "Epoch   6/30 Batch   38/41 - Loss:  3.460, Seconds: 1.12\n",
            "Epoch   6/30 Batch   40/41 - Loss:  2.151, Seconds: 1.12\n",
            "Average loss for this update: 3.058\n",
            "New Record!\n",
            "Epoch   7/30 Batch    2/41 - Loss:  4.004, Seconds: 0.92\n",
            "Epoch   7/30 Batch    4/41 - Loss:  2.355, Seconds: 0.85\n",
            "Epoch   7/30 Batch    6/41 - Loss:  2.215, Seconds: 0.64\n",
            "Epoch   7/30 Batch    8/41 - Loss:  3.165, Seconds: 1.32\n",
            "Epoch   7/30 Batch   10/41 - Loss:  2.790, Seconds: 0.77\n",
            "Epoch   7/30 Batch   12/41 - Loss:  2.565, Seconds: 1.00\n",
            "Epoch   7/30 Batch   14/41 - Loss:  3.440, Seconds: 1.05\n",
            "Epoch   7/30 Batch   16/41 - Loss:  2.885, Seconds: 0.71\n",
            "Epoch   7/30 Batch   18/41 - Loss:  1.481, Seconds: 0.73\n",
            "Epoch   7/30 Batch   20/41 - Loss:  3.063, Seconds: 1.07\n",
            "Epoch   7/30 Batch   22/41 - Loss:  1.650, Seconds: 0.98\n",
            "Epoch   7/30 Batch   24/41 - Loss:  2.148, Seconds: 0.89\n",
            "Epoch   7/30 Batch   26/41 - Loss:  2.504, Seconds: 0.72\n",
            "Epoch   7/30 Batch   28/41 - Loss:  3.159, Seconds: 0.98\n",
            "Epoch   7/30 Batch   30/41 - Loss:  1.962, Seconds: 0.83\n",
            "Epoch   7/30 Batch   32/41 - Loss:  2.336, Seconds: 0.75\n",
            "Epoch   7/30 Batch   34/41 - Loss:  1.841, Seconds: 0.92\n",
            "Epoch   7/30 Batch   36/41 - Loss:  1.542, Seconds: 0.83\n",
            "Epoch   7/30 Batch   38/41 - Loss:  2.194, Seconds: 1.00\n",
            "Epoch   7/30 Batch   40/41 - Loss:  1.476, Seconds: 1.01\n",
            "Average loss for this update: 2.439\n",
            "New Record!\n",
            "Epoch   8/30 Batch    2/41 - Loss:  3.720, Seconds: 0.87\n",
            "Epoch   8/30 Batch    4/41 - Loss:  1.764, Seconds: 0.80\n",
            "Epoch   8/30 Batch    6/41 - Loss:  2.558, Seconds: 0.57\n",
            "Epoch   8/30 Batch    8/41 - Loss:  2.215, Seconds: 1.30\n",
            "Epoch   8/30 Batch   10/41 - Loss:  2.561, Seconds: 0.72\n",
            "Epoch   8/30 Batch   12/41 - Loss:  1.820, Seconds: 0.91\n",
            "Epoch   8/30 Batch   14/41 - Loss:  3.161, Seconds: 0.97\n",
            "Epoch   8/30 Batch   16/41 - Loss:  2.975, Seconds: 0.67\n",
            "Epoch   8/30 Batch   18/41 - Loss:  0.996, Seconds: 0.67\n",
            "Epoch   8/30 Batch   20/41 - Loss:  3.360, Seconds: 0.99\n",
            "Epoch   8/30 Batch   22/41 - Loss:  1.354, Seconds: 0.94\n",
            "Epoch   8/30 Batch   24/41 - Loss:  1.431, Seconds: 0.83\n",
            "Epoch   8/30 Batch   26/41 - Loss:  1.570, Seconds: 0.68\n",
            "Epoch   8/30 Batch   28/41 - Loss:  2.555, Seconds: 0.93\n",
            "Epoch   8/30 Batch   30/41 - Loss:  1.920, Seconds: 0.78\n",
            "Epoch   8/30 Batch   32/41 - Loss:  1.563, Seconds: 0.70\n",
            "Epoch   8/30 Batch   34/41 - Loss:  1.189, Seconds: 0.84\n",
            "Epoch   8/30 Batch   36/41 - Loss:  1.946, Seconds: 0.75\n",
            "Epoch   8/30 Batch   38/41 - Loss:  2.160, Seconds: 0.93\n",
            "Epoch   8/30 Batch   40/41 - Loss:  1.857, Seconds: 0.92\n",
            "Average loss for this update: 2.134\n",
            "New Record!\n",
            "Epoch   9/30 Batch    2/41 - Loss:  3.841, Seconds: 0.88\n",
            "Epoch   9/30 Batch    4/41 - Loss:  1.928, Seconds: 0.81\n",
            "Epoch   9/30 Batch    6/41 - Loss:  1.633, Seconds: 0.59\n",
            "Epoch   9/30 Batch    8/41 - Loss:  2.008, Seconds: 1.29\n",
            "Epoch   9/30 Batch   10/41 - Loss:  2.160, Seconds: 0.74\n",
            "Epoch   9/30 Batch   12/41 - Loss:  2.130, Seconds: 0.90\n",
            "Epoch   9/30 Batch   14/41 - Loss:  2.940, Seconds: 0.97\n",
            "Epoch   9/30 Batch   16/41 - Loss:  2.850, Seconds: 0.67\n",
            "Epoch   9/30 Batch   18/41 - Loss:  1.006, Seconds: 0.66\n",
            "Epoch   9/30 Batch   20/41 - Loss:  3.134, Seconds: 1.00\n",
            "Epoch   9/30 Batch   22/41 - Loss:  1.536, Seconds: 0.92\n",
            "Epoch   9/30 Batch   24/41 - Loss:  1.684, Seconds: 0.85\n",
            "Epoch   9/30 Batch   26/41 - Loss:  1.007, Seconds: 0.70\n",
            "Epoch   9/30 Batch   28/41 - Loss:  2.914, Seconds: 0.93\n",
            "Epoch   9/30 Batch   30/41 - Loss:  1.801, Seconds: 0.79\n",
            "Epoch   9/30 Batch   32/41 - Loss:  1.282, Seconds: 0.70\n",
            "Epoch   9/30 Batch   34/41 - Loss:  1.286, Seconds: 0.85\n",
            "Epoch   9/30 Batch   36/41 - Loss:  1.116, Seconds: 0.80\n",
            "Epoch   9/30 Batch   38/41 - Loss:  2.738, Seconds: 0.95\n",
            "Epoch   9/30 Batch   40/41 - Loss:  1.551, Seconds: 0.93\n",
            "Average loss for this update: 2.027\n",
            "New Record!\n",
            "Epoch  10/30 Batch    2/41 - Loss:  2.706, Seconds: 0.87\n",
            "Epoch  10/30 Batch    4/41 - Loss:  1.212, Seconds: 0.80\n",
            "Epoch  10/30 Batch    6/41 - Loss:  2.271, Seconds: 0.57\n",
            "Epoch  10/30 Batch    8/41 - Loss:  1.700, Seconds: 1.29\n",
            "Epoch  10/30 Batch   10/41 - Loss:  2.015, Seconds: 0.74\n",
            "Epoch  10/30 Batch   12/41 - Loss:  2.231, Seconds: 0.90\n",
            "Epoch  10/30 Batch   14/41 - Loss:  2.200, Seconds: 0.97\n",
            "Epoch  10/30 Batch   16/41 - Loss:  1.460, Seconds: 0.67\n",
            "Epoch  10/30 Batch   18/41 - Loss:  1.360, Seconds: 0.67\n",
            "Epoch  10/30 Batch   20/41 - Loss:  2.529, Seconds: 1.00\n",
            "Epoch  10/30 Batch   22/41 - Loss:  1.238, Seconds: 0.90\n",
            "Epoch  10/30 Batch   24/41 - Loss:  1.586, Seconds: 0.84\n",
            "Epoch  10/30 Batch   26/41 - Loss:  1.139, Seconds: 0.68\n",
            "Epoch  10/30 Batch   28/41 - Loss:  2.410, Seconds: 0.91\n",
            "Epoch  10/30 Batch   30/41 - Loss:  2.072, Seconds: 0.75\n",
            "Epoch  10/30 Batch   32/41 - Loss:  0.790, Seconds: 0.72\n",
            "Epoch  10/30 Batch   34/41 - Loss:  0.820, Seconds: 0.84\n",
            "Epoch  10/30 Batch   36/41 - Loss:  1.212, Seconds: 0.77\n",
            "Epoch  10/30 Batch   38/41 - Loss:  2.101, Seconds: 0.93\n",
            "Epoch  10/30 Batch   40/41 - Loss:  1.467, Seconds: 0.91\n",
            "Average loss for this update: 1.726\n",
            "New Record!\n",
            "Epoch  11/30 Batch    2/41 - Loss:  2.598, Seconds: 0.86\n",
            "Epoch  11/30 Batch    4/41 - Loss:  0.931, Seconds: 0.80\n",
            "Epoch  11/30 Batch    6/41 - Loss:  1.049, Seconds: 0.55\n",
            "Epoch  11/30 Batch    8/41 - Loss:  1.457, Seconds: 1.28\n",
            "Epoch  11/30 Batch   10/41 - Loss:  1.612, Seconds: 0.75\n",
            "Epoch  11/30 Batch   12/41 - Loss:  1.472, Seconds: 0.87\n",
            "Epoch  11/30 Batch   14/41 - Loss:  1.598, Seconds: 0.96\n",
            "Epoch  11/30 Batch   16/41 - Loss:  0.998, Seconds: 0.67\n",
            "Epoch  11/30 Batch   18/41 - Loss:  0.500, Seconds: 0.63\n",
            "Epoch  11/30 Batch   20/41 - Loss:  2.394, Seconds: 0.99\n",
            "Epoch  11/30 Batch   22/41 - Loss:  0.810, Seconds: 0.93\n",
            "Epoch  11/30 Batch   24/41 - Loss:  0.656, Seconds: 0.84\n",
            "Epoch  11/30 Batch   26/41 - Loss:  0.886, Seconds: 0.67\n",
            "Epoch  11/30 Batch   28/41 - Loss:  2.262, Seconds: 0.89\n",
            "Epoch  11/30 Batch   30/41 - Loss:  1.461, Seconds: 0.76\n",
            "Epoch  11/30 Batch   32/41 - Loss:  0.689, Seconds: 0.72\n",
            "Epoch  11/30 Batch   34/41 - Loss:  1.287, Seconds: 0.84\n",
            "Epoch  11/30 Batch   36/41 - Loss:  0.846, Seconds: 0.77\n",
            "Epoch  11/30 Batch   38/41 - Loss:  1.661, Seconds: 0.93\n",
            "Epoch  11/30 Batch   40/41 - Loss:  1.218, Seconds: 0.95\n",
            "Average loss for this update: 1.319\n",
            "New Record!\n",
            "Epoch  12/30 Batch    2/41 - Loss:  2.141, Seconds: 0.84\n",
            "Epoch  12/30 Batch    4/41 - Loss:  0.937, Seconds: 0.83\n",
            "Epoch  12/30 Batch    6/41 - Loss:  0.681, Seconds: 0.57\n",
            "Epoch  12/30 Batch    8/41 - Loss:  1.193, Seconds: 1.29\n",
            "Epoch  12/30 Batch   10/41 - Loss:  1.634, Seconds: 0.76\n",
            "Epoch  12/30 Batch   12/41 - Loss:  1.308, Seconds: 0.91\n",
            "Epoch  12/30 Batch   14/41 - Loss:  1.247, Seconds: 1.00\n",
            "Epoch  12/30 Batch   16/41 - Loss:  0.700, Seconds: 0.66\n",
            "Epoch  12/30 Batch   18/41 - Loss:  0.303, Seconds: 0.68\n",
            "Epoch  12/30 Batch   20/41 - Loss:  1.668, Seconds: 0.98\n",
            "Epoch  12/30 Batch   22/41 - Loss:  0.586, Seconds: 0.92\n",
            "Epoch  12/30 Batch   24/41 - Loss:  1.099, Seconds: 0.86\n",
            "Epoch  12/30 Batch   26/41 - Loss:  0.505, Seconds: 0.67\n",
            "Epoch  12/30 Batch   28/41 - Loss:  1.542, Seconds: 0.93\n",
            "Epoch  12/30 Batch   30/41 - Loss:  1.050, Seconds: 0.78\n",
            "Epoch  12/30 Batch   32/41 - Loss:  0.443, Seconds: 0.69\n",
            "Epoch  12/30 Batch   34/41 - Loss:  0.649, Seconds: 0.84\n",
            "Epoch  12/30 Batch   36/41 - Loss:  0.798, Seconds: 0.78\n",
            "Epoch  12/30 Batch   38/41 - Loss:  0.980, Seconds: 0.94\n",
            "Epoch  12/30 Batch   40/41 - Loss:  1.358, Seconds: 0.94\n",
            "Average loss for this update: 1.041\n",
            "New Record!\n",
            "Epoch  13/30 Batch    2/41 - Loss:  1.347, Seconds: 0.89\n",
            "Epoch  13/30 Batch    4/41 - Loss:  0.641, Seconds: 0.83\n",
            "Epoch  13/30 Batch    6/41 - Loss:  0.518, Seconds: 0.56\n",
            "Epoch  13/30 Batch    8/41 - Loss:  0.904, Seconds: 1.28\n",
            "Epoch  13/30 Batch   10/41 - Loss:  1.189, Seconds: 0.74\n",
            "Epoch  13/30 Batch   12/41 - Loss:  1.011, Seconds: 0.90\n",
            "Epoch  13/30 Batch   14/41 - Loss:  1.179, Seconds: 1.00\n",
            "Epoch  13/30 Batch   16/41 - Loss:  0.746, Seconds: 0.67\n",
            "Epoch  13/30 Batch   18/41 - Loss:  0.085, Seconds: 0.66\n",
            "Epoch  13/30 Batch   20/41 - Loss:  1.509, Seconds: 1.00\n",
            "Epoch  13/30 Batch   22/41 - Loss:  0.270, Seconds: 0.93\n",
            "Epoch  13/30 Batch   24/41 - Loss:  0.872, Seconds: 0.86\n",
            "Epoch  13/30 Batch   26/41 - Loss:  0.452, Seconds: 0.65\n",
            "Epoch  13/30 Batch   28/41 - Loss:  1.111, Seconds: 0.94\n",
            "Epoch  13/30 Batch   30/41 - Loss:  0.782, Seconds: 0.76\n",
            "Epoch  13/30 Batch   32/41 - Loss:  0.197, Seconds: 0.70\n",
            "Epoch  13/30 Batch   34/41 - Loss:  0.622, Seconds: 0.83\n",
            "Epoch  13/30 Batch   36/41 - Loss:  0.342, Seconds: 0.77\n",
            "Epoch  13/30 Batch   38/41 - Loss:  0.651, Seconds: 0.92\n",
            "Epoch  13/30 Batch   40/41 - Loss:  0.562, Seconds: 0.94\n",
            "Average loss for this update: 0.749\n",
            "New Record!\n",
            "Epoch  14/30 Batch    2/41 - Loss:  1.072, Seconds: 0.85\n",
            "Epoch  14/30 Batch    4/41 - Loss:  0.643, Seconds: 0.82\n",
            "Epoch  14/30 Batch    6/41 - Loss:  0.453, Seconds: 0.58\n",
            "Epoch  14/30 Batch    8/41 - Loss:  0.780, Seconds: 1.30\n",
            "Epoch  14/30 Batch   10/41 - Loss:  0.614, Seconds: 0.76\n",
            "Epoch  14/30 Batch   12/41 - Loss:  0.660, Seconds: 0.91\n",
            "Epoch  14/30 Batch   14/41 - Loss:  0.687, Seconds: 0.97\n",
            "Epoch  14/30 Batch   16/41 - Loss:  0.882, Seconds: 0.67\n",
            "Epoch  14/30 Batch   18/41 - Loss:  0.061, Seconds: 0.66\n",
            "Epoch  14/30 Batch   20/41 - Loss:  1.142, Seconds: 0.98\n",
            "Epoch  14/30 Batch   22/41 - Loss:  0.387, Seconds: 0.87\n",
            "Epoch  14/30 Batch   24/41 - Loss:  0.651, Seconds: 0.85\n",
            "Epoch  14/30 Batch   26/41 - Loss:  0.363, Seconds: 0.69\n",
            "Epoch  14/30 Batch   28/41 - Loss:  0.725, Seconds: 0.92\n",
            "Epoch  14/30 Batch   30/41 - Loss:  0.707, Seconds: 0.76\n",
            "Epoch  14/30 Batch   32/41 - Loss:  0.250, Seconds: 0.69\n",
            "Epoch  14/30 Batch   34/41 - Loss:  0.525, Seconds: 0.84\n",
            "Epoch  14/30 Batch   36/41 - Loss:  0.160, Seconds: 0.79\n",
            "Epoch  14/30 Batch   38/41 - Loss:  0.562, Seconds: 0.92\n",
            "Epoch  14/30 Batch   40/41 - Loss:  0.509, Seconds: 0.92\n",
            "Average loss for this update: 0.592\n",
            "New Record!\n",
            "Epoch  15/30 Batch    2/41 - Loss:  0.857, Seconds: 0.89\n",
            "Epoch  15/30 Batch    4/41 - Loss:  0.673, Seconds: 0.78\n",
            "Epoch  15/30 Batch    6/41 - Loss:  0.332, Seconds: 0.57\n",
            "Epoch  15/30 Batch    8/41 - Loss:  0.742, Seconds: 1.28\n",
            "Epoch  15/30 Batch   10/41 - Loss:  0.561, Seconds: 0.74\n",
            "Epoch  15/30 Batch   12/41 - Loss:  0.545, Seconds: 0.91\n",
            "Epoch  15/30 Batch   14/41 - Loss:  0.636, Seconds: 0.97\n",
            "Epoch  15/30 Batch   16/41 - Loss:  0.829, Seconds: 0.67\n",
            "Epoch  15/30 Batch   18/41 - Loss:  0.048, Seconds: 0.65\n",
            "Epoch  15/30 Batch   20/41 - Loss:  1.035, Seconds: 1.02\n",
            "Epoch  15/30 Batch   22/41 - Loss:  0.366, Seconds: 0.92\n",
            "Epoch  15/30 Batch   24/41 - Loss:  0.413, Seconds: 0.84\n",
            "Epoch  15/30 Batch   26/41 - Loss:  0.154, Seconds: 0.68\n",
            "Epoch  15/30 Batch   28/41 - Loss:  0.696, Seconds: 0.92\n",
            "Epoch  15/30 Batch   30/41 - Loss:  0.442, Seconds: 0.79\n",
            "Epoch  15/30 Batch   32/41 - Loss:  0.130, Seconds: 0.72\n",
            "Epoch  15/30 Batch   34/41 - Loss:  0.225, Seconds: 0.82\n",
            "Epoch  15/30 Batch   36/41 - Loss:  0.188, Seconds: 0.77\n",
            "Epoch  15/30 Batch   38/41 - Loss:  0.373, Seconds: 0.94\n",
            "Epoch  15/30 Batch   40/41 - Loss:  0.480, Seconds: 0.94\n",
            "Average loss for this update: 0.486\n",
            "New Record!\n",
            "Epoch  16/30 Batch    2/41 - Loss:  0.659, Seconds: 0.89\n",
            "Epoch  16/30 Batch    4/41 - Loss:  0.425, Seconds: 0.80\n",
            "Epoch  16/30 Batch    6/41 - Loss:  0.237, Seconds: 0.56\n",
            "Epoch  16/30 Batch    8/41 - Loss:  0.652, Seconds: 1.26\n",
            "Epoch  16/30 Batch   10/41 - Loss:  0.484, Seconds: 0.73\n",
            "Epoch  16/30 Batch   12/41 - Loss:  0.467, Seconds: 0.92\n",
            "Epoch  16/30 Batch   14/41 - Loss:  0.390, Seconds: 1.01\n",
            "Epoch  16/30 Batch   16/41 - Loss:  0.438, Seconds: 0.68\n",
            "Epoch  16/30 Batch   18/41 - Loss:  0.030, Seconds: 0.64\n",
            "Epoch  16/30 Batch   20/41 - Loss:  1.145, Seconds: 0.98\n",
            "Epoch  16/30 Batch   22/41 - Loss:  0.224, Seconds: 0.90\n",
            "Epoch  16/30 Batch   24/41 - Loss:  0.230, Seconds: 0.83\n",
            "Epoch  16/30 Batch   26/41 - Loss:  0.045, Seconds: 0.69\n",
            "Epoch  16/30 Batch   28/41 - Loss:  0.842, Seconds: 0.92\n",
            "Epoch  16/30 Batch   30/41 - Loss:  0.314, Seconds: 0.76\n",
            "Epoch  16/30 Batch   32/41 - Loss:  0.046, Seconds: 0.70\n",
            "Epoch  16/30 Batch   34/41 - Loss:  0.100, Seconds: 0.86\n",
            "Epoch  16/30 Batch   36/41 - Loss:  0.185, Seconds: 0.76\n",
            "Epoch  16/30 Batch   38/41 - Loss:  0.187, Seconds: 0.92\n",
            "Epoch  16/30 Batch   40/41 - Loss:  0.473, Seconds: 0.93\n",
            "Average loss for this update: 0.379\n",
            "New Record!\n",
            "Epoch  17/30 Batch    2/41 - Loss:  0.587, Seconds: 0.85\n",
            "Epoch  17/30 Batch    4/41 - Loss:  0.369, Seconds: 0.79\n",
            "Epoch  17/30 Batch    6/41 - Loss:  0.209, Seconds: 0.57\n",
            "Epoch  17/30 Batch    8/41 - Loss:  0.666, Seconds: 1.26\n",
            "Epoch  17/30 Batch   10/41 - Loss:  0.417, Seconds: 0.75\n",
            "Epoch  17/30 Batch   12/41 - Loss:  0.292, Seconds: 0.90\n",
            "Epoch  17/30 Batch   14/41 - Loss:  0.475, Seconds: 0.98\n",
            "Epoch  17/30 Batch   16/41 - Loss:  0.358, Seconds: 0.66\n",
            "Epoch  17/30 Batch   18/41 - Loss:  0.023, Seconds: 0.69\n",
            "Epoch  17/30 Batch   20/41 - Loss:  0.710, Seconds: 1.00\n",
            "Epoch  17/30 Batch   22/41 - Loss:  0.155, Seconds: 0.90\n",
            "Epoch  17/30 Batch   24/41 - Loss:  0.134, Seconds: 0.83\n",
            "Epoch  17/30 Batch   26/41 - Loss:  0.032, Seconds: 0.70\n",
            "Epoch  17/30 Batch   28/41 - Loss:  0.856, Seconds: 0.95\n",
            "Epoch  17/30 Batch   30/41 - Loss:  0.369, Seconds: 0.76\n",
            "Epoch  17/30 Batch   32/41 - Loss:  0.036, Seconds: 0.70\n",
            "Epoch  17/30 Batch   34/41 - Loss:  0.071, Seconds: 0.84\n",
            "Epoch  17/30 Batch   36/41 - Loss:  0.106, Seconds: 0.78\n",
            "Epoch  17/30 Batch   38/41 - Loss:  0.216, Seconds: 0.92\n",
            "Epoch  17/30 Batch   40/41 - Loss:  0.287, Seconds: 0.93\n",
            "Average loss for this update: 0.318\n",
            "New Record!\n",
            "Epoch  18/30 Batch    2/41 - Loss:  0.468, Seconds: 0.87\n",
            "Epoch  18/30 Batch    4/41 - Loss:  0.259, Seconds: 0.79\n",
            "Epoch  18/30 Batch    6/41 - Loss:  0.407, Seconds: 0.57\n",
            "Epoch  18/30 Batch    8/41 - Loss:  0.690, Seconds: 1.28\n",
            "Epoch  18/30 Batch   10/41 - Loss:  0.434, Seconds: 0.72\n",
            "Epoch  18/30 Batch   12/41 - Loss:  0.335, Seconds: 0.91\n",
            "Epoch  18/30 Batch   14/41 - Loss:  0.493, Seconds: 0.97\n",
            "Epoch  18/30 Batch   16/41 - Loss:  0.466, Seconds: 0.68\n",
            "Epoch  18/30 Batch   18/41 - Loss:  0.026, Seconds: 0.66\n",
            "Epoch  18/30 Batch   20/41 - Loss:  0.634, Seconds: 0.98\n",
            "Epoch  18/30 Batch   22/41 - Loss:  0.084, Seconds: 0.90\n",
            "Epoch  18/30 Batch   24/41 - Loss:  0.125, Seconds: 0.82\n",
            "Epoch  18/30 Batch   26/41 - Loss:  0.026, Seconds: 0.68\n",
            "Epoch  18/30 Batch   28/41 - Loss:  0.596, Seconds: 0.92\n",
            "Epoch  18/30 Batch   30/41 - Loss:  0.373, Seconds: 0.74\n",
            "Epoch  18/30 Batch   32/41 - Loss:  0.026, Seconds: 0.70\n",
            "Epoch  18/30 Batch   34/41 - Loss:  0.058, Seconds: 0.85\n",
            "Epoch  18/30 Batch   36/41 - Loss:  0.030, Seconds: 0.76\n",
            "Epoch  18/30 Batch   38/41 - Loss:  0.159, Seconds: 0.95\n",
            "Epoch  18/30 Batch   40/41 - Loss:  0.171, Seconds: 0.93\n",
            "Average loss for this update: 0.293\n",
            "New Record!\n",
            "Epoch  19/30 Batch    2/41 - Loss:  0.176, Seconds: 0.85\n",
            "Epoch  19/30 Batch    4/41 - Loss:  0.110, Seconds: 0.79\n",
            "Epoch  19/30 Batch    6/41 - Loss:  0.186, Seconds: 0.56\n",
            "Epoch  19/30 Batch    8/41 - Loss:  0.637, Seconds: 1.26\n",
            "Epoch  19/30 Batch   10/41 - Loss:  0.378, Seconds: 0.74\n",
            "Epoch  19/30 Batch   12/41 - Loss:  0.248, Seconds: 0.87\n",
            "Epoch  19/30 Batch   14/41 - Loss:  0.258, Seconds: 0.98\n",
            "Epoch  19/30 Batch   16/41 - Loss:  0.236, Seconds: 0.67\n",
            "Epoch  19/30 Batch   18/41 - Loss:  0.018, Seconds: 0.66\n",
            "Epoch  19/30 Batch   20/41 - Loss:  0.491, Seconds: 1.00\n",
            "Epoch  19/30 Batch   22/41 - Loss:  0.064, Seconds: 0.90\n",
            "Epoch  19/30 Batch   24/41 - Loss:  0.049, Seconds: 0.83\n",
            "Epoch  19/30 Batch   26/41 - Loss:  0.020, Seconds: 0.67\n",
            "Epoch  19/30 Batch   28/41 - Loss:  0.588, Seconds: 0.91\n",
            "Epoch  19/30 Batch   30/41 - Loss:  0.235, Seconds: 0.75\n",
            "Epoch  19/30 Batch   32/41 - Loss:  0.041, Seconds: 0.69\n",
            "Epoch  19/30 Batch   34/41 - Loss:  0.046, Seconds: 0.81\n",
            "Epoch  19/30 Batch   36/41 - Loss:  0.025, Seconds: 0.75\n",
            "Epoch  19/30 Batch   38/41 - Loss:  0.202, Seconds: 0.93\n",
            "Epoch  19/30 Batch   40/41 - Loss:  0.128, Seconds: 0.93\n",
            "Average loss for this update: 0.207\n",
            "New Record!\n",
            "Epoch  20/30 Batch    2/41 - Loss:  0.193, Seconds: 0.86\n",
            "Epoch  20/30 Batch    4/41 - Loss:  0.106, Seconds: 0.81\n",
            "Epoch  20/30 Batch    6/41 - Loss:  0.069, Seconds: 0.55\n",
            "Epoch  20/30 Batch    8/41 - Loss:  0.421, Seconds: 1.26\n",
            "Epoch  20/30 Batch   10/41 - Loss:  0.279, Seconds: 0.73\n",
            "Epoch  20/30 Batch   12/41 - Loss:  0.141, Seconds: 0.87\n",
            "Epoch  20/30 Batch   14/41 - Loss:  0.237, Seconds: 0.96\n",
            "Epoch  20/30 Batch   16/41 - Loss:  0.141, Seconds: 0.68\n",
            "Epoch  20/30 Batch   18/41 - Loss:  0.014, Seconds: 0.67\n",
            "Epoch  20/30 Batch   20/41 - Loss:  0.489, Seconds: 0.98\n",
            "Epoch  20/30 Batch   22/41 - Loss:  0.093, Seconds: 0.91\n",
            "Epoch  20/30 Batch   24/41 - Loss:  0.032, Seconds: 0.83\n",
            "Epoch  20/30 Batch   26/41 - Loss:  0.024, Seconds: 0.66\n",
            "Epoch  20/30 Batch   28/41 - Loss:  0.426, Seconds: 0.93\n",
            "Epoch  20/30 Batch   30/41 - Loss:  0.172, Seconds: 0.76\n",
            "Epoch  20/30 Batch   32/41 - Loss:  0.028, Seconds: 0.68\n",
            "Epoch  20/30 Batch   34/41 - Loss:  0.044, Seconds: 0.84\n",
            "Epoch  20/30 Batch   36/41 - Loss:  0.027, Seconds: 0.75\n",
            "Epoch  20/30 Batch   38/41 - Loss:  0.139, Seconds: 0.92\n",
            "Epoch  20/30 Batch   40/41 - Loss:  0.174, Seconds: 0.93\n",
            "Average loss for this update: 0.162\n",
            "New Record!\n",
            "Epoch  21/30 Batch    2/41 - Loss:  0.130, Seconds: 0.85\n",
            "Epoch  21/30 Batch    4/41 - Loss:  0.051, Seconds: 0.84\n",
            "Epoch  21/30 Batch    6/41 - Loss:  0.060, Seconds: 0.59\n",
            "Epoch  21/30 Batch    8/41 - Loss:  0.410, Seconds: 1.31\n",
            "Epoch  21/30 Batch   10/41 - Loss:  0.261, Seconds: 0.73\n",
            "Epoch  21/30 Batch   12/41 - Loss:  0.099, Seconds: 0.89\n",
            "Epoch  21/30 Batch   14/41 - Loss:  0.221, Seconds: 0.96\n",
            "Epoch  21/30 Batch   16/41 - Loss:  0.083, Seconds: 0.65\n",
            "Epoch  21/30 Batch   18/41 - Loss:  0.012, Seconds: 0.68\n",
            "Epoch  21/30 Batch   20/41 - Loss:  0.288, Seconds: 1.00\n",
            "Epoch  21/30 Batch   22/41 - Loss:  0.059, Seconds: 0.89\n",
            "Epoch  21/30 Batch   24/41 - Loss:  0.021, Seconds: 0.82\n",
            "Epoch  21/30 Batch   26/41 - Loss:  0.015, Seconds: 0.67\n",
            "Epoch  21/30 Batch   28/41 - Loss:  0.328, Seconds: 0.90\n",
            "Epoch  21/30 Batch   30/41 - Loss:  0.150, Seconds: 0.76\n",
            "Epoch  21/30 Batch   32/41 - Loss:  0.019, Seconds: 0.70\n",
            "Epoch  21/30 Batch   34/41 - Loss:  0.022, Seconds: 0.84\n",
            "Epoch  21/30 Batch   36/41 - Loss:  0.018, Seconds: 0.77\n",
            "Epoch  21/30 Batch   38/41 - Loss:  0.135, Seconds: 0.92\n",
            "Epoch  21/30 Batch   40/41 - Loss:  0.465, Seconds: 0.92\n",
            "Average loss for this update: 0.142\n",
            "New Record!\n",
            "Epoch  22/30 Batch    2/41 - Loss:  0.097, Seconds: 0.83\n",
            "Epoch  22/30 Batch    4/41 - Loss:  0.043, Seconds: 0.78\n",
            "Epoch  22/30 Batch    6/41 - Loss:  0.037, Seconds: 0.55\n",
            "Epoch  22/30 Batch    8/41 - Loss:  0.334, Seconds: 1.25\n",
            "Epoch  22/30 Batch   10/41 - Loss:  0.248, Seconds: 0.73\n",
            "Epoch  22/30 Batch   12/41 - Loss:  0.113, Seconds: 0.89\n",
            "Epoch  22/30 Batch   14/41 - Loss:  0.256, Seconds: 0.94\n",
            "Epoch  22/30 Batch   16/41 - Loss:  0.078, Seconds: 0.64\n",
            "Epoch  22/30 Batch   18/41 - Loss:  0.011, Seconds: 0.65\n",
            "Epoch  22/30 Batch   20/41 - Loss:  0.311, Seconds: 0.96\n",
            "Epoch  22/30 Batch   22/41 - Loss:  0.022, Seconds: 0.88\n",
            "Epoch  22/30 Batch   24/41 - Loss:  0.021, Seconds: 0.81\n",
            "Epoch  22/30 Batch   26/41 - Loss:  0.016, Seconds: 0.66\n",
            "Epoch  22/30 Batch   28/41 - Loss:  0.296, Seconds: 0.89\n",
            "Epoch  22/30 Batch   30/41 - Loss:  0.226, Seconds: 0.77\n",
            "Epoch  22/30 Batch   32/41 - Loss:  0.017, Seconds: 0.67\n",
            "Epoch  22/30 Batch   34/41 - Loss:  0.028, Seconds: 0.84\n",
            "Epoch  22/30 Batch   36/41 - Loss:  0.020, Seconds: 0.76\n",
            "Epoch  22/30 Batch   38/41 - Loss:  0.257, Seconds: 0.92\n",
            "Epoch  22/30 Batch   40/41 - Loss:  0.412, Seconds: 0.92\n",
            "Average loss for this update: 0.142\n",
            "New Record!\n",
            "Epoch  23/30 Batch    2/41 - Loss:  0.126, Seconds: 0.84\n",
            "Epoch  23/30 Batch    4/41 - Loss:  0.037, Seconds: 0.80\n",
            "Epoch  23/30 Batch    6/41 - Loss:  0.024, Seconds: 0.58\n",
            "Epoch  23/30 Batch    8/41 - Loss:  0.277, Seconds: 1.27\n",
            "Epoch  23/30 Batch   10/41 - Loss:  0.163, Seconds: 0.76\n",
            "Epoch  23/30 Batch   12/41 - Loss:  0.077, Seconds: 0.91\n",
            "Epoch  23/30 Batch   14/41 - Loss:  0.196, Seconds: 0.95\n",
            "Epoch  23/30 Batch   16/41 - Loss:  0.061, Seconds: 0.65\n",
            "Epoch  23/30 Batch   18/41 - Loss:  0.008, Seconds: 0.66\n",
            "Epoch  23/30 Batch   20/41 - Loss:  0.191, Seconds: 0.97\n",
            "Epoch  23/30 Batch   22/41 - Loss:  0.020, Seconds: 0.91\n",
            "Epoch  23/30 Batch   24/41 - Loss:  0.018, Seconds: 0.80\n",
            "Epoch  23/30 Batch   26/41 - Loss:  0.014, Seconds: 0.69\n",
            "Epoch  23/30 Batch   28/41 - Loss:  0.169, Seconds: 0.94\n",
            "Epoch  23/30 Batch   30/41 - Loss:  0.166, Seconds: 0.79\n",
            "Epoch  23/30 Batch   32/41 - Loss:  0.014, Seconds: 0.70\n",
            "Epoch  23/30 Batch   34/41 - Loss:  0.024, Seconds: 0.83\n",
            "Epoch  23/30 Batch   36/41 - Loss:  0.014, Seconds: 0.79\n",
            "Epoch  23/30 Batch   38/41 - Loss:  0.026, Seconds: 0.93\n",
            "Epoch  23/30 Batch   40/41 - Loss:  0.132, Seconds: 0.92\n",
            "Average loss for this update: 0.088\n",
            "New Record!\n",
            "Epoch  24/30 Batch    2/41 - Loss:  0.126, Seconds: 0.87\n",
            "Epoch  24/30 Batch    4/41 - Loss:  0.025, Seconds: 0.82\n",
            "Epoch  24/30 Batch    6/41 - Loss:  0.029, Seconds: 0.57\n",
            "Epoch  24/30 Batch    8/41 - Loss:  0.253, Seconds: 1.22\n",
            "Epoch  24/30 Batch   10/41 - Loss:  0.136, Seconds: 0.74\n",
            "Epoch  24/30 Batch   12/41 - Loss:  0.030, Seconds: 0.90\n",
            "Epoch  24/30 Batch   14/41 - Loss:  0.067, Seconds: 0.98\n",
            "Epoch  24/30 Batch   16/41 - Loss:  0.049, Seconds: 0.66\n",
            "Epoch  24/30 Batch   18/41 - Loss:  0.010, Seconds: 0.67\n",
            "Epoch  24/30 Batch   20/41 - Loss:  0.105, Seconds: 1.02\n",
            "Epoch  24/30 Batch   22/41 - Loss:  0.021, Seconds: 0.95\n",
            "Epoch  24/30 Batch   24/41 - Loss:  0.017, Seconds: 0.83\n",
            "Epoch  24/30 Batch   26/41 - Loss:  0.010, Seconds: 0.68\n",
            "Epoch  24/30 Batch   28/41 - Loss:  0.082, Seconds: 0.92\n",
            "Epoch  24/30 Batch   30/41 - Loss:  0.085, Seconds: 0.77\n",
            "Epoch  24/30 Batch   32/41 - Loss:  0.012, Seconds: 0.72\n",
            "Epoch  24/30 Batch   34/41 - Loss:  0.017, Seconds: 0.85\n",
            "Epoch  24/30 Batch   36/41 - Loss:  0.011, Seconds: 0.77\n",
            "Epoch  24/30 Batch   38/41 - Loss:  0.025, Seconds: 0.92\n",
            "Epoch  24/30 Batch   40/41 - Loss:  0.016, Seconds: 0.93\n",
            "Average loss for this update: 0.056\n",
            "New Record!\n",
            "Epoch  25/30 Batch    2/41 - Loss:  0.133, Seconds: 0.87\n",
            "Epoch  25/30 Batch    4/41 - Loss:  0.024, Seconds: 0.79\n",
            "Epoch  25/30 Batch    6/41 - Loss:  0.027, Seconds: 0.59\n",
            "Epoch  25/30 Batch    8/41 - Loss:  0.291, Seconds: 1.27\n",
            "Epoch  25/30 Batch   10/41 - Loss:  0.147, Seconds: 0.74\n",
            "Epoch  25/30 Batch   12/41 - Loss:  0.022, Seconds: 0.90\n",
            "Epoch  25/30 Batch   14/41 - Loss:  0.071, Seconds: 0.97\n",
            "Epoch  25/30 Batch   16/41 - Loss:  0.030, Seconds: 0.67\n",
            "Epoch  25/30 Batch   18/41 - Loss:  0.008, Seconds: 0.73\n",
            "Epoch  25/30 Batch   20/41 - Loss:  0.109, Seconds: 1.00\n",
            "Epoch  25/30 Batch   22/41 - Loss:  0.016, Seconds: 0.91\n",
            "Epoch  25/30 Batch   24/41 - Loss:  0.015, Seconds: 0.86\n",
            "Epoch  25/30 Batch   26/41 - Loss:  0.010, Seconds: 0.70\n",
            "Epoch  25/30 Batch   28/41 - Loss:  0.073, Seconds: 0.94\n",
            "Epoch  25/30 Batch   30/41 - Loss:  0.080, Seconds: 0.80\n",
            "Epoch  25/30 Batch   32/41 - Loss:  0.009, Seconds: 0.73\n",
            "Epoch  25/30 Batch   34/41 - Loss:  0.013, Seconds: 0.85\n",
            "Epoch  25/30 Batch   36/41 - Loss:  0.010, Seconds: 0.75\n",
            "Epoch  25/30 Batch   38/41 - Loss:  0.027, Seconds: 0.93\n",
            "Epoch  25/30 Batch   40/41 - Loss:  0.021, Seconds: 0.95\n",
            "Average loss for this update: 0.057\n",
            "No Improvement.\n",
            "Epoch  26/30 Batch    2/41 - Loss:  0.039, Seconds: 0.86\n",
            "Epoch  26/30 Batch    4/41 - Loss:  0.017, Seconds: 0.83\n",
            "Epoch  26/30 Batch    6/41 - Loss:  0.021, Seconds: 0.56\n",
            "Epoch  26/30 Batch    8/41 - Loss:  0.262, Seconds: 1.28\n",
            "Epoch  26/30 Batch   10/41 - Loss:  0.096, Seconds: 0.74\n",
            "Epoch  26/30 Batch   12/41 - Loss:  0.021, Seconds: 0.91\n",
            "Epoch  26/30 Batch   14/41 - Loss:  0.039, Seconds: 1.01\n",
            "Epoch  26/30 Batch   16/41 - Loss:  0.019, Seconds: 0.66\n",
            "Epoch  26/30 Batch   18/41 - Loss:  0.009, Seconds: 0.67\n",
            "Epoch  26/30 Batch   20/41 - Loss:  0.058, Seconds: 0.99\n",
            "Epoch  26/30 Batch   22/41 - Loss:  0.012, Seconds: 0.91\n",
            "Epoch  26/30 Batch   24/41 - Loss:  0.011, Seconds: 0.83\n",
            "Epoch  26/30 Batch   26/41 - Loss:  0.008, Seconds: 0.68\n",
            "Epoch  26/30 Batch   28/41 - Loss:  0.093, Seconds: 0.95\n",
            "Epoch  26/30 Batch   30/41 - Loss:  0.037, Seconds: 0.78\n",
            "Epoch  26/30 Batch   32/41 - Loss:  0.011, Seconds: 0.72\n",
            "Epoch  26/30 Batch   34/41 - Loss:  0.014, Seconds: 0.90\n",
            "Epoch  26/30 Batch   36/41 - Loss:  0.011, Seconds: 0.79\n",
            "Epoch  26/30 Batch   38/41 - Loss:  0.021, Seconds: 0.95\n",
            "Epoch  26/30 Batch   40/41 - Loss:  0.021, Seconds: 0.96\n",
            "Average loss for this update: 0.041\n",
            "New Record!\n",
            "Epoch  27/30 Batch    2/41 - Loss:  0.028, Seconds: 0.86\n",
            "Epoch  27/30 Batch    4/41 - Loss:  0.014, Seconds: 0.84\n",
            "Epoch  27/30 Batch    6/41 - Loss:  0.017, Seconds: 0.64\n",
            "Epoch  27/30 Batch    8/41 - Loss:  0.232, Seconds: 1.29\n",
            "Epoch  27/30 Batch   10/41 - Loss:  0.076, Seconds: 0.73\n",
            "Epoch  27/30 Batch   12/41 - Loss:  0.017, Seconds: 0.90\n",
            "Epoch  27/30 Batch   14/41 - Loss:  0.022, Seconds: 0.97\n",
            "Epoch  27/30 Batch   16/41 - Loss:  0.022, Seconds: 0.69\n",
            "Epoch  27/30 Batch   18/41 - Loss:  0.006, Seconds: 0.70\n",
            "Epoch  27/30 Batch   20/41 - Loss:  0.076, Seconds: 1.15\n",
            "Epoch  27/30 Batch   22/41 - Loss:  0.012, Seconds: 0.91\n",
            "Epoch  27/30 Batch   24/41 - Loss:  0.013, Seconds: 0.83\n",
            "Epoch  27/30 Batch   26/41 - Loss:  0.007, Seconds: 0.71\n",
            "Epoch  27/30 Batch   28/41 - Loss:  0.049, Seconds: 0.95\n",
            "Epoch  27/30 Batch   30/41 - Loss:  0.016, Seconds: 0.80\n",
            "Epoch  27/30 Batch   32/41 - Loss:  0.008, Seconds: 0.69\n",
            "Epoch  27/30 Batch   34/41 - Loss:  0.011, Seconds: 0.84\n",
            "Epoch  27/30 Batch   36/41 - Loss:  0.008, Seconds: 0.78\n",
            "Epoch  27/30 Batch   38/41 - Loss:  0.010, Seconds: 0.93\n",
            "Epoch  27/30 Batch   40/41 - Loss:  0.012, Seconds: 0.93\n",
            "Average loss for this update: 0.033\n",
            "New Record!\n",
            "Epoch  28/30 Batch    2/41 - Loss:  0.041, Seconds: 0.89\n",
            "Epoch  28/30 Batch    4/41 - Loss:  0.013, Seconds: 0.80\n",
            "Epoch  28/30 Batch    6/41 - Loss:  0.016, Seconds: 0.56\n",
            "Epoch  28/30 Batch    8/41 - Loss:  0.156, Seconds: 1.31\n",
            "Epoch  28/30 Batch   10/41 - Loss:  0.027, Seconds: 0.74\n",
            "Epoch  28/30 Batch   12/41 - Loss:  0.018, Seconds: 0.90\n",
            "Epoch  28/30 Batch   14/41 - Loss:  0.023, Seconds: 0.98\n",
            "Epoch  28/30 Batch   16/41 - Loss:  0.017, Seconds: 0.66\n",
            "Epoch  28/30 Batch   18/41 - Loss:  0.007, Seconds: 0.65\n",
            "Epoch  28/30 Batch   20/41 - Loss:  0.069, Seconds: 0.96\n",
            "Epoch  28/30 Batch   22/41 - Loss:  0.009, Seconds: 0.91\n",
            "Epoch  28/30 Batch   24/41 - Loss:  0.007, Seconds: 0.83\n",
            "Epoch  28/30 Batch   26/41 - Loss:  0.008, Seconds: 0.65\n",
            "Epoch  28/30 Batch   28/41 - Loss:  0.046, Seconds: 0.91\n",
            "Epoch  28/30 Batch   30/41 - Loss:  0.014, Seconds: 0.76\n",
            "Epoch  28/30 Batch   32/41 - Loss:  0.008, Seconds: 0.69\n",
            "Epoch  28/30 Batch   34/41 - Loss:  0.010, Seconds: 0.82\n",
            "Epoch  28/30 Batch   36/41 - Loss:  0.008, Seconds: 0.76\n",
            "Epoch  28/30 Batch   38/41 - Loss:  0.017, Seconds: 0.94\n",
            "Epoch  28/30 Batch   40/41 - Loss:  0.011, Seconds: 0.91\n",
            "Average loss for this update: 0.026\n",
            "New Record!\n",
            "Epoch  29/30 Batch    2/41 - Loss:  0.029, Seconds: 0.85\n",
            "Epoch  29/30 Batch    4/41 - Loss:  0.012, Seconds: 0.81\n",
            "Epoch  29/30 Batch    6/41 - Loss:  0.012, Seconds: 0.58\n",
            "Epoch  29/30 Batch    8/41 - Loss:  0.078, Seconds: 1.25\n",
            "Epoch  29/30 Batch   10/41 - Loss:  0.027, Seconds: 0.73\n",
            "Epoch  29/30 Batch   12/41 - Loss:  0.014, Seconds: 0.93\n",
            "Epoch  29/30 Batch   14/41 - Loss:  0.018, Seconds: 0.97\n",
            "Epoch  29/30 Batch   16/41 - Loss:  0.013, Seconds: 0.67\n",
            "Epoch  29/30 Batch   18/41 - Loss:  0.005, Seconds: 0.66\n",
            "Epoch  29/30 Batch   20/41 - Loss:  0.032, Seconds: 0.98\n",
            "Epoch  29/30 Batch   22/41 - Loss:  0.011, Seconds: 0.91\n",
            "Epoch  29/30 Batch   24/41 - Loss:  0.008, Seconds: 0.82\n",
            "Epoch  29/30 Batch   26/41 - Loss:  0.007, Seconds: 0.67\n",
            "Epoch  29/30 Batch   28/41 - Loss:  0.020, Seconds: 0.90\n",
            "Epoch  29/30 Batch   30/41 - Loss:  0.014, Seconds: 0.76\n",
            "Epoch  29/30 Batch   32/41 - Loss:  0.006, Seconds: 0.69\n",
            "Epoch  29/30 Batch   34/41 - Loss:  0.008, Seconds: 0.83\n",
            "Epoch  29/30 Batch   36/41 - Loss:  0.007, Seconds: 0.76\n",
            "Epoch  29/30 Batch   38/41 - Loss:  0.014, Seconds: 0.94\n",
            "Epoch  29/30 Batch   40/41 - Loss:  0.008, Seconds: 0.96\n",
            "Average loss for this update: 0.017\n",
            "New Record!\n",
            "Epoch  30/30 Batch    2/41 - Loss:  0.023, Seconds: 0.83\n",
            "Epoch  30/30 Batch    4/41 - Loss:  0.012, Seconds: 0.77\n",
            "Epoch  30/30 Batch    6/41 - Loss:  0.011, Seconds: 0.56\n",
            "Epoch  30/30 Batch    8/41 - Loss:  0.060, Seconds: 1.26\n",
            "Epoch  30/30 Batch   10/41 - Loss:  0.021, Seconds: 0.69\n",
            "Epoch  30/30 Batch   12/41 - Loss:  0.013, Seconds: 0.89\n",
            "Epoch  30/30 Batch   14/41 - Loss:  0.018, Seconds: 0.95\n",
            "Epoch  30/30 Batch   16/41 - Loss:  0.011, Seconds: 0.65\n",
            "Epoch  30/30 Batch   18/41 - Loss:  0.005, Seconds: 0.65\n",
            "Epoch  30/30 Batch   20/41 - Loss:  0.026, Seconds: 0.91\n",
            "Epoch  30/30 Batch   22/41 - Loss:  0.008, Seconds: 0.89\n",
            "Epoch  30/30 Batch   24/41 - Loss:  0.009, Seconds: 0.82\n",
            "Epoch  30/30 Batch   26/41 - Loss:  0.006, Seconds: 0.66\n",
            "Epoch  30/30 Batch   28/41 - Loss:  0.020, Seconds: 0.89\n",
            "Epoch  30/30 Batch   30/41 - Loss:  0.011, Seconds: 0.75\n",
            "Epoch  30/30 Batch   32/41 - Loss:  0.006, Seconds: 0.69\n",
            "Epoch  30/30 Batch   34/41 - Loss:  0.007, Seconds: 0.85\n",
            "Epoch  30/30 Batch   36/41 - Loss:  0.007, Seconds: 0.74\n",
            "Epoch  30/30 Batch   38/41 - Loss:  0.009, Seconds: 0.90\n",
            "Epoch  30/30 Batch   40/41 - Loss:  0.008, Seconds: 0.93\n",
            "Average loss for this update: 0.015\n",
            "New Record!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rhi0ZIrQUKL"
      },
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    text = clean_text(text)\n",
        "    \n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvVahjfwRzNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4106985a-4d03-477a-9a95-71fe7861e07d"
      },
      "source": [
        "random = np.random.randint(0,len(clean_texts))\n",
        "input_sentence = clean_texts[random]\n",
        "text = text_to_seq(clean_texts[random])\n",
        "\n",
        "checkpoint = \"model1.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding from the tweet\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', df.Text[random])\n",
        "print('Original summary:', df.Summary[random])#clean_summaries[random]\n",
        "\n",
        "print('\\nText')\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model1.ckpt\n",
            "Original Text: হাইকোর্ট বিভাগের বিচারপতিদের নিয়ে নবনিযুক্ত প্রধান বিচারপতির ফুলকোর্ট সভা অনুষ্ঠিত হয়েছে। সভায় নিম্ন আদালতে দায়িত্বপালনরত ১৩২ বিচারকের পদোন্নতির সিদ্ধান্ত হয়েছে।\n",
            "Original summary: ১৩২ বিচারকের পদোন্নতির সিদ্ধান্ত\n",
            "\n",
            "Text\n",
            "  Input Words: হাইকোর্ট বিভাগের বিচারপতিদের নিয়ে নবনিযুক্ত প্রধান বিচারপতির <UNK> সভা অনুষ্ঠিত হয়েছে সভায় নিম্ন আদালতে দায়িত্বপালনরত বিচারকের পদোন্নতির সিদ্ধান্ত হয়েছে\n",
            "\n",
            "Summary\n",
            "Response Words: বিচারকের পদোন্নতির সিদ্ধান্ত\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDqB9iCkR3t8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b5ca67-12de-4de3-e707-dee87989d9a2"
      },
      "source": [
        "input_sentence = input()\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "checkpoint = \"model1.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('\\nText')\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "হাইকোর্ট বিভাগের বিচারপতিদের নিয়ে নবনিযুক্ত প্রধান বিচারপতির <UNK> সভা অনুষ্ঠিত হয়েছে সভায় নিম্ন আদালতে দায়িত্বপালনরত বিচারকের পদোন্নতির সিদ্ধান্ত হয়েছে\n",
            "INFO:tensorflow:Restoring parameters from model1.ckpt\n",
            "\n",
            "Text\n",
            "  Input Words: হাইকোর্ট বিভাগের বিচারপতিদের নিয়ে নবনিযুক্ত প্রধান বিচারপতির সভা অনুষ্ঠিত হয়েছে সভায় নিম্ন আদালতে দায়িত্বপালনরত বিচারকের পদোন্নতির সিদ্ধান্ত হয়েছে\n",
            "\n",
            "Summary\n",
            "Response Words: বিচারকের পদোন্নতির সিদ্ধান্ত\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvD_t1K7Z1Jg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}